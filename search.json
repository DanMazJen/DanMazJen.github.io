[
  {
    "objectID": "blog/2025-11-20-ATE-ATT-ATU-CATE.html",
    "href": "blog/2025-11-20-ATE-ATT-ATU-CATE.html",
    "title": "Estimating causal effects using a simplified R implementation",
    "section": "",
    "text": "I recently participated in a causal inference class and realized how difficult it is for some of my students and colleagues to actually compute causal inference estimates. Although the most important point is to understand when and why to use these methods, I also think accessibility to the computational part could be helpful for many.\n\n\n\n\n\nMeme 1\n\n\n\n\n\n\nMeme 2\n\n\n\n\nIn this short blog, I’ll introduce four key estimands that can be computed using a framework called G-computation:\n\nthe average treatment effect (ATE)\nthe average treatment effect on the treated (ATT)\nthe average treatment effect on the untreated (ATU)\nthe conditional average treatment effect (CATE)\n\n\n\nThe Average Treatment Effect (ATE) measures how the treatment impacts outcomes across the entire study population. It addresses the question: Would it be beneficial to offer this program or treatment to everyone?\nThe Average Treatment Effect on the Treated (ATT) focuses on the effect of the treatment specifically for those participants who actually received it. It helps evaluate: Should the treatment be continued for the group currently receiving it?\nThe Average Treatment Effect on the Untreated (ATU) examines the effect of the treatment on those who did not receive it. This helps answer: Would it be advantageous to expand the program or treatment to individuals who were initially excluded?\nThe Conditional Average Treatment Effect (CATE) estimates how the treatment affects outcomes for a specific subgroup of the population, defined by certain characteristics (e.g., age, gender, or baseline risk). It answers the question: How does the treatment work for people with particular traits or conditions?\n\n\n\nWhen estimating causal effects, such as the ATE or CATE, four fundamental assumptions must hold to ensure valid conclusions:\nExchangeability (or no unmeasured confounding) – This assumes that, after accounting for observed variables, the treated and untreated groups are comparable. In other words, there are no hidden factors that systematically affect both treatment assignment and the outcome.\nPositivity (or overlap) – Every individual in the study population must have a nonzero chance of receiving each treatment option. Without this, it becomes impossible to estimate the effect for some groups because they never experience one of the treatments.\nNon-interference (or the Stable Unit Treatment Value Assumption, SUTVA) – One person’s outcome should not be influenced by whether someone else receives the treatment. Each participant’s outcome depends only on their own treatment status.\nConsistency – The treatment is well-defined, and each individual’s observed outcome under the treatment they actually received is the same as the outcome that would be observed under the same treatment condition in theory.\n\n\n\n\n\n\n\nCode\n# keep only ctrl and trt1\ndat &lt;- dplyr::filter(ChickWeight, Time %in% c(0, 12)) |&gt;\n  #dplyr::filter(Diet %in% c(1, 3, 4)) |&gt;\n  dplyr::mutate(\n    treatment = dplyr::if_else(Diet == '1', '1', '2')\n  ) |&gt;\n  dplyr::mutate(timepoint = dplyr::if_else(Time == 0, \"pre\", \"post\")) |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(Chick, treatment), # Diet not needed...\n    names_from = timepoint,\n    values_from = weight\n  )\n\n#dat$Diet &lt;- as.factor(dat$Diet)\ndat$treatment &lt;- as.factor(dat$treatment)\n#dat$study_arm &lt;- dat$group\n\n#dat$treatment &lt;- forcats::fct_relevel(c('1', '2'), c('ctrl', 'trt1'))\n#dat$group &lt;- c()\n\n\n\n\n\nFirst, we fit a statistical model which controls for confounders. Then, we use the fitted model to “predict” or “impute” what would happen to an individual under alternative treatment scenarios. Finally, we compare counterfactual predictions to derive an estimate of the treatment effect (Hernán and Robins 2020).\nTo get an adjustment set for confounder control, we need to make a DAG\n::: {.cell}\n\nCode\nour_dag &lt;- dagitty::dagitty(\n \"dag {\n pre_weight -&gt; post_weight\n treatment  -&gt; post_weight\n}\"\n)\n\nplot(our_dag)\n\n::: {.cell-output .cell-output-stderr}\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n:::\n::: {.cell-output-display}  ::: :::\nThus, we will be using the pre-weight as a confounding variable for the modeling.\n\n\nCode\n# model without adjustment set\nunadj_mod &lt;- lm(post ~ treatment, data = dat)\nunadj_mod\n\n\n\nCall:\nlm(formula = post ~ treatment, data = dat)\n\nCoefficients:\n(Intercept)   treatment2  \n     108.53        33.84  \n\n\nCode\n# model with adjustment set\nadj_mod &lt;- lm(post ~ treatment * pre, data = dat)\nadj_mod\n\n\n\nCall:\nlm(formula = post ~ treatment * pre, data = dat)\n\nCoefficients:\n   (Intercept)      treatment2             pre  treatment2:pre  \n        14.835         283.241           2.256          -6.069  \n\n\nNote that this model does not correct for adherence. Therefore, it is the intention-to-treat effect. If we adjusted for adherence or in any other way try to correct for individuals deviating from the protocol, this would give the per-protocol effect.\n\n\n\nNow, we want to go from a “missing data”-problem, where only some individuals got the treatment while the others got no treatment, to a full matrix of all individuals got both treated and not. Of course, this is not what happened in real life. This is counter to the fact (i.e. counterfactual) estimates, which we can predict using normal regression coefficients. It looks something like this:\n\n\nCode\n# Show first 3 subjects with outcome and only being treated or not treater with missing values on the other\ndat |&gt;\n  # create subject id\n  dplyr::mutate(ID = dplyr::row_number()) |&gt;\n  # spread so each subject has columns for both treatments\n  tidyr::pivot_wider(\n    id_cols = ID,\n    names_from = treatment,\n    values_from = post,\n    values_fill = NA\n  ) |&gt;\n  dplyr::slice(c(1, 20, 35, 40)) # .preserve = group\n\n\n# A tibble: 4 × 3\n     ID   `1`   `2`\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1   106    NA\n2    20    77    NA\n3    35    NA   201\n4    40    NA   154\n\n\nCode\n# Show subjects with outcome using marginaleffect imputation\ncounter_factuals &lt;- marginaleffects::predictions(\n  adj_mod,\n  variables = \"treatment\"\n)\n\ncounter_factual_matrix &lt;- counter_factuals |&gt;\n  # ensure we have a subject index:\n  tibble::as_tibble() |&gt;\n  #dplyr::mutate(Subject = dplyr::row_number()) |&gt;\n  dplyr::filter(rowidcf %in% c(1, 20, 35, 40)) |&gt;\n  dplyr::mutate(treatment = dplyr::if_else(rowid &gt; 40, 'Y_j^1', 'Y_j^0')) |&gt;\n  dplyr::select(treatment, rowidcf, estimate) |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(rowidcf),\n    names_from = treatment,\n    values_from = estimate\n  )\n\nknitr::kable(\n  counter_factual_matrix,\n  caption = \"Predicted weights for Subjects 1, 20, 35, and 40 under Treatment = 0 and 1\"\n)\n\n\n\nPredicted weights for Subjects 1, 20, 35, and 40 under Treatment = 0 and 1\n\n\nrowidcf\nY_j^0\nY_j^1\n\n\n\n\n1\n109.5950\n137.9178\n\n\n20\n105.0826\n145.5444\n\n\n35\n102.8264\n149.3577\n\n\n40\n109.5950\n137.9178\n\n\n\n\n\nWe are only interested in the average effect, as individual treatment effects are not possible to estimate.\n::: {.cell}\n\nCode\nmarginaleffects::avg_predictions(\n adj_mod,\n variables = \"treatment\",\n by = \"treatment\"\n)\n\n::: {.cell-output .cell-output-stdout}\n\n treatment Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n         1      108       7.89 13.6   &lt;0.001 138.3  92.1    123\n         2      141       5.72 24.7   &lt;0.001 445.4 130.1    153\n\nType: response\n::: :::\nremember that the ATE is the difference, which we can get by using the ´avg_comparisons()´ of {marginaleffects}\n::: {.cell}\n\nCode\nATE = marginaleffects::avg_comparisons(\n adj_mod,\n variables = \"treatment\",\n newdata = dat\n)\nATE\n\n::: {.cell-output .cell-output-stdout}\n\n Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n       34       9.86 3.45   &lt;0.001 10.8  14.7   53.3\n\nTerm: treatment\nType: response\nComparison: 2 - 1\n::: :::\nThe ATT\n::: {.cell}\n\nCode\nATT &lt;- marginaleffects::avg_comparisons(\n adj_mod,\n variables = \"treatment\",\n newdata = subset(treatment == '2')\n)\nATT\n\n::: {.cell-output .cell-output-stdout}\n\n Estimate Std. Error   z Pr(&gt;|z|)    S 2.5 % 97.5 %\n     35.4       10.7 3.3   &lt;0.001 10.0  14.4   56.4\n\nTerm: treatment\nType: response\nComparison: 2 - 1\n::: :::\nand the ATU\n::: {.cell}\n\nCode\nATU &lt;- marginaleffects::avg_comparisons(\n adj_mod,\n variables = \"treatment\",\n newdata = subset(treatment == '1')\n)\nATU\n\n::: {.cell-output .cell-output-stdout}\n\n Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n     31.2       9.55 3.27  0.00109 9.8  12.5   49.9\n\nTerm: treatment\nType: response\nComparison: 2 - 1\n::: :::\nThe CATE.\n::: {.cell}\n\nCode\nCATE &lt;- marginaleffects::avg_comparisons(\n adj_mod,\n variables = \"treatment\",\n by = \"pre\"\n)\nCATE\n\n::: {.cell-output .cell-output-stdout}\n\n pre Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n  39     46.5       25.0 1.86   0.0626  4.0  -2.45   95.5\n  40     40.5       16.3 2.48   0.0132  6.2   8.47   72.5\n  41     34.4       10.0 3.42   &lt;0.001 10.7  14.70   54.1\n  42     28.3       11.3 2.50   0.0123  6.3   6.15   50.5\n  43     22.3       18.6 1.19   0.2327  2.1 -14.29   58.8\n\nTerm: treatment\nType: response\nComparison: 2 - 1\n::: :::\nNote that we gain some interesting information here. We see, as would be expected, that chicks with low pre-weight (susceptible to being underweight) will gain more weight, whereas chicks with initial high pre-weight (robust or previliged) will gain less weight."
  },
  {
    "objectID": "blog/2025-11-20-ATE-ATT-ATU-CATE.html#the-estimand",
    "href": "blog/2025-11-20-ATE-ATT-ATU-CATE.html#the-estimand",
    "title": "Estimating causal effects using a simplified R implementation",
    "section": "",
    "text": "The Average Treatment Effect (ATE) measures how the treatment impacts outcomes across the entire study population. It addresses the question: Would it be beneficial to offer this program or treatment to everyone?\nThe Average Treatment Effect on the Treated (ATT) focuses on the effect of the treatment specifically for those participants who actually received it. It helps evaluate: Should the treatment be continued for the group currently receiving it?\nThe Average Treatment Effect on the Untreated (ATU) examines the effect of the treatment on those who did not receive it. This helps answer: Would it be advantageous to expand the program or treatment to individuals who were initially excluded?\nThe Conditional Average Treatment Effect (CATE) estimates how the treatment affects outcomes for a specific subgroup of the population, defined by certain characteristics (e.g., age, gender, or baseline risk). It answers the question: How does the treatment work for people with particular traits or conditions?"
  },
  {
    "objectID": "blog/2025-11-20-ATE-ATT-ATU-CATE.html#the-assumptions",
    "href": "blog/2025-11-20-ATE-ATT-ATU-CATE.html#the-assumptions",
    "title": "Estimating causal effects using a simplified R implementation",
    "section": "",
    "text": "When estimating causal effects, such as the ATE or CATE, four fundamental assumptions must hold to ensure valid conclusions:\nExchangeability (or no unmeasured confounding) – This assumes that, after accounting for observed variables, the treated and untreated groups are comparable. In other words, there are no hidden factors that systematically affect both treatment assignment and the outcome.\nPositivity (or overlap) – Every individual in the study population must have a nonzero chance of receiving each treatment option. Without this, it becomes impossible to estimate the effect for some groups because they never experience one of the treatments.\nNon-interference (or the Stable Unit Treatment Value Assumption, SUTVA) – One person’s outcome should not be influenced by whether someone else receives the treatment. Each participant’s outcome depends only on their own treatment status.\nConsistency – The treatment is well-defined, and each individual’s observed outcome under the treatment they actually received is the same as the outcome that would be observed under the same treatment condition in theory."
  },
  {
    "objectID": "blog/2025-11-20-ATE-ATT-ATU-CATE.html#the-implementation-in-r",
    "href": "blog/2025-11-20-ATE-ATT-ATU-CATE.html#the-implementation-in-r",
    "title": "Estimating causal effects using a simplified R implementation",
    "section": "",
    "text": "Code\n# keep only ctrl and trt1\ndat &lt;- dplyr::filter(ChickWeight, Time %in% c(0, 12)) |&gt;\n  #dplyr::filter(Diet %in% c(1, 3, 4)) |&gt;\n  dplyr::mutate(\n    treatment = dplyr::if_else(Diet == '1', '1', '2')\n  ) |&gt;\n  dplyr::mutate(timepoint = dplyr::if_else(Time == 0, \"pre\", \"post\")) |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(Chick, treatment), # Diet not needed...\n    names_from = timepoint,\n    values_from = weight\n  )\n\n#dat$Diet &lt;- as.factor(dat$Diet)\ndat$treatment &lt;- as.factor(dat$treatment)\n#dat$study_arm &lt;- dat$group\n\n#dat$treatment &lt;- forcats::fct_relevel(c('1', '2'), c('ctrl', 'trt1'))\n#dat$group &lt;- c()\n\n\n\n\n\nFirst, we fit a statistical model which controls for confounders. Then, we use the fitted model to “predict” or “impute” what would happen to an individual under alternative treatment scenarios. Finally, we compare counterfactual predictions to derive an estimate of the treatment effect (Hernán and Robins 2020).\nTo get an adjustment set for confounder control, we need to make a DAG\n::: {.cell}\n\nCode\nour_dag &lt;- dagitty::dagitty(\n \"dag {\n pre_weight -&gt; post_weight\n treatment  -&gt; post_weight\n}\"\n)\n\nplot(our_dag)\n\n::: {.cell-output .cell-output-stderr}\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n:::\n::: {.cell-output-display}  ::: :::\nThus, we will be using the pre-weight as a confounding variable for the modeling.\n\n\nCode\n# model without adjustment set\nunadj_mod &lt;- lm(post ~ treatment, data = dat)\nunadj_mod\n\n\n\nCall:\nlm(formula = post ~ treatment, data = dat)\n\nCoefficients:\n(Intercept)   treatment2  \n     108.53        33.84  \n\n\nCode\n# model with adjustment set\nadj_mod &lt;- lm(post ~ treatment * pre, data = dat)\nadj_mod\n\n\n\nCall:\nlm(formula = post ~ treatment * pre, data = dat)\n\nCoefficients:\n   (Intercept)      treatment2             pre  treatment2:pre  \n        14.835         283.241           2.256          -6.069  \n\n\nNote that this model does not correct for adherence. Therefore, it is the intention-to-treat effect. If we adjusted for adherence or in any other way try to correct for individuals deviating from the protocol, this would give the per-protocol effect.\n\n\n\nNow, we want to go from a “missing data”-problem, where only some individuals got the treatment while the others got no treatment, to a full matrix of all individuals got both treated and not. Of course, this is not what happened in real life. This is counter to the fact (i.e. counterfactual) estimates, which we can predict using normal regression coefficients. It looks something like this:\n\n\nCode\n# Show first 3 subjects with outcome and only being treated or not treater with missing values on the other\ndat |&gt;\n  # create subject id\n  dplyr::mutate(ID = dplyr::row_number()) |&gt;\n  # spread so each subject has columns for both treatments\n  tidyr::pivot_wider(\n    id_cols = ID,\n    names_from = treatment,\n    values_from = post,\n    values_fill = NA\n  ) |&gt;\n  dplyr::slice(c(1, 20, 35, 40)) # .preserve = group\n\n\n# A tibble: 4 × 3\n     ID   `1`   `2`\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1   106    NA\n2    20    77    NA\n3    35    NA   201\n4    40    NA   154\n\n\nCode\n# Show subjects with outcome using marginaleffect imputation\ncounter_factuals &lt;- marginaleffects::predictions(\n  adj_mod,\n  variables = \"treatment\"\n)\n\ncounter_factual_matrix &lt;- counter_factuals |&gt;\n  # ensure we have a subject index:\n  tibble::as_tibble() |&gt;\n  #dplyr::mutate(Subject = dplyr::row_number()) |&gt;\n  dplyr::filter(rowidcf %in% c(1, 20, 35, 40)) |&gt;\n  dplyr::mutate(treatment = dplyr::if_else(rowid &gt; 40, 'Y_j^1', 'Y_j^0')) |&gt;\n  dplyr::select(treatment, rowidcf, estimate) |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(rowidcf),\n    names_from = treatment,\n    values_from = estimate\n  )\n\nknitr::kable(\n  counter_factual_matrix,\n  caption = \"Predicted weights for Subjects 1, 20, 35, and 40 under Treatment = 0 and 1\"\n)\n\n\n\nPredicted weights for Subjects 1, 20, 35, and 40 under Treatment = 0 and 1\n\n\nrowidcf\nY_j^0\nY_j^1\n\n\n\n\n1\n109.5950\n137.9178\n\n\n20\n105.0826\n145.5444\n\n\n35\n102.8264\n149.3577\n\n\n40\n109.5950\n137.9178\n\n\n\n\n\nWe are only interested in the average effect, as individual treatment effects are not possible to estimate.\n::: {.cell}\n\nCode\nmarginaleffects::avg_predictions(\n adj_mod,\n variables = \"treatment\",\n by = \"treatment\"\n)\n\n::: {.cell-output .cell-output-stdout}\n\n treatment Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n         1      108       7.89 13.6   &lt;0.001 138.3  92.1    123\n         2      141       5.72 24.7   &lt;0.001 445.4 130.1    153\n\nType: response\n::: :::\nremember that the ATE is the difference, which we can get by using the ´avg_comparisons()´ of {marginaleffects}\n::: {.cell}\n\nCode\nATE = marginaleffects::avg_comparisons(\n adj_mod,\n variables = \"treatment\",\n newdata = dat\n)\nATE\n\n::: {.cell-output .cell-output-stdout}\n\n Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n       34       9.86 3.45   &lt;0.001 10.8  14.7   53.3\n\nTerm: treatment\nType: response\nComparison: 2 - 1\n::: :::\nThe ATT\n::: {.cell}\n\nCode\nATT &lt;- marginaleffects::avg_comparisons(\n adj_mod,\n variables = \"treatment\",\n newdata = subset(treatment == '2')\n)\nATT\n\n::: {.cell-output .cell-output-stdout}\n\n Estimate Std. Error   z Pr(&gt;|z|)    S 2.5 % 97.5 %\n     35.4       10.7 3.3   &lt;0.001 10.0  14.4   56.4\n\nTerm: treatment\nType: response\nComparison: 2 - 1\n::: :::\nand the ATU\n::: {.cell}\n\nCode\nATU &lt;- marginaleffects::avg_comparisons(\n adj_mod,\n variables = \"treatment\",\n newdata = subset(treatment == '1')\n)\nATU\n\n::: {.cell-output .cell-output-stdout}\n\n Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n     31.2       9.55 3.27  0.00109 9.8  12.5   49.9\n\nTerm: treatment\nType: response\nComparison: 2 - 1\n::: :::\nThe CATE.\n::: {.cell}\n\nCode\nCATE &lt;- marginaleffects::avg_comparisons(\n adj_mod,\n variables = \"treatment\",\n by = \"pre\"\n)\nCATE\n\n::: {.cell-output .cell-output-stdout}\n\n pre Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n  39     46.5       25.0 1.86   0.0626  4.0  -2.45   95.5\n  40     40.5       16.3 2.48   0.0132  6.2   8.47   72.5\n  41     34.4       10.0 3.42   &lt;0.001 10.7  14.70   54.1\n  42     28.3       11.3 2.50   0.0123  6.3   6.15   50.5\n  43     22.3       18.6 1.19   0.2327  2.1 -14.29   58.8\n\nTerm: treatment\nType: response\nComparison: 2 - 1\n::: :::\nNote that we gain some interesting information here. We see, as would be expected, that chicks with low pre-weight (susceptible to being underweight) will gain more weight, whereas chicks with initial high pre-weight (robust or previliged) will gain less weight."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog!\nI’ll now try to write more stuff. This should help me identify if it works…\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nEstimating causal effects using a simplified R implementation\n\n\nHow {marginaleffects} can make your causal inferece scripts readable\n\n\n\nR\n\nmarginaleffects\n\ncausal inference\n\nATE\n\n\n\nUsing marginaleffects, I show how to estimate ATE, ATT, ATU, and CATE in an RCT study\n\n\n\n\n\nNov 20, 2025\n\n\nDaniel S. Mazhari-Jensen\n\n\n\n\n\n\n\n\n\n\n\n\nWhy random slopes matters…\n\n\n… and why random intercepts are often not enough\n\n\n\nR\n\nlme4\n\nmultilevel regression\n\n\n\nUsing lme4, I show why random slopes is often needed in biomedical research and experiments\n\n\n\n\n\nJul 23, 2025\n\n\nDaniel S. Mazhari-Jensen\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to My Blog\n\n\n\n\n\n\nGeneral\n\n\n\nThis is my first blog post.\n\n\n\n\n\nJul 22, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Daniel — welcome to my little corner of the internet.\nI’m a health science researcher with a growing interest in data modeling, reproducible analysis, and the R ecosystem. I also have a deep appreciation for specialty coffee — especially as fuel for long modeling sessions and debugging marathons.\nThis site is a mix of things: answers to questions friends and colleagues ask me, experiments with methods or tools I want to learn better, and thoughts on tech and research practices that catch my interest. You might find posts about multilevel models with lme4, visualizations in ggplot2, or just a useful little R trick I want to remember later.\nIf you’re curious about something I post — or if you have a better way to do it — feel free to reach out. I like sharing ideas, and I’m always learning.\nThanks for stopping by."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Daniel — welcome to my little corner of the internet.\nI’m a health science researcher with a growing interest in data modeling, reproducible analysis, and the R ecosystem. I also have a deep appreciation for specialty coffee — especially as fuel for long modeling sessions and debugging marathons.\nThis site is a mix of things: answers to questions friends and colleagues ask me, experiments with methods or tools I want to learn better, and thoughts on tech and research practices that catch my interest. You might find posts about multilevel models with lme4, visualizations in ggplot2, or just a useful little R trick I want to remember later.\nIf you’re curious about something I post — or if you have a better way to do it — feel free to reach out. I like sharing ideas, and I’m always learning.\nThanks for stopping by."
  },
  {
    "objectID": "about.html#about-the-site",
    "href": "about.html#about-the-site",
    "title": "About",
    "section": "About the site",
    "text": "About the site\nCurrently, this is a test. The site will expand over the next couple of months.\nThe content will be small blog-styled pieces of statistical material that friends and colleagues recently started asking me. Some of the material will also be for my own amusement, relating to topics I’m actively exploring and trying to get a better felling of."
  },
  {
    "objectID": "about.html#my-background",
    "href": "about.html#my-background",
    "title": "About",
    "section": "My Background",
    "text": "My Background\nAalborg University | Denmark | PhD-fellow enrolled to Biomedical Engineering and Neuroscience | 2022 - active\nMaastricht University | Netherlands | RMSc in Cognitive and Clinical Neuroscience | 2020 - 2022\nUniversity College of Northern Denmark (UCN) | Denmark Lecturer | 2018 - 2020\nAalborg University | Denmark | BA and MA in Music Therapy | 2013 - 2018"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DanMazJen.github.io",
    "section": "",
    "text": "Welcome to the website.\nCurrently, this is a test. The site will expand over the next couple of months.\nThe content will be small blog-styled pieces of statistical material."
  },
  {
    "objectID": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html",
    "href": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "In this post, we’ll discuss why random slopes are often of special concern in health science data and biometrics. We will use lme4 and ggplot2 to do so. The data set we’ll use to demonstrate this comes from …\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\nCode\nset.seed(314)\n\n# Parameters\nn_subjects &lt;- 30\nn_sessions &lt;- 5\n\n# Fixed effect of session: general learning effect\nbeta_0 &lt;- 600   # Average baseline RT in ms\nbeta_1 &lt;- -10   # Average decrease in RT per session\n\n# Random effects: standard deviations\nsd_intercept &lt;- 50    # SD of baseline RTs across subjects\nsd_slope     &lt;- 25    # SD of learning rates (slopes)\nrho          &lt;- 0.2   # Correlation between intercepts and slopes\n\n# Within-subject residual error\nsigma_error &lt;- 30\n\n# Construct subject-level random effects (correlated)\nsubject_re &lt;- MASS::mvrnorm(\n  n = n_subjects,\n  mu = c(0, 0),\n  Sigma = matrix(c(sd_intercept^2, rho*sd_intercept*sd_slope,\n                   rho*sd_intercept*sd_slope, sd_slope^2), 2)\n) |&gt; \n  as.data.frame() |&gt;\n  setNames(c(\"intercept_re\", \"slope_re\")) |&gt;\n  dplyr::mutate(subject = factor(1:n_subjects))\n\n# Create session vector repeated for each subject\nsession &lt;- rep(0:(n_sessions - 1), times = n_subjects)\n\n# Repeat subject IDs for each session\nsubject &lt;- rep(subject_re$subject, each = n_sessions)\n\n# Join the random effects\nsim_data &lt;- tibble::tibble(subject = subject, session = session) |&gt;\n  dplyr::left_join(subject_re, by = \"subject\") |&gt;\n  dplyr::mutate(\n    reaction_time = beta_0 + intercept_re +\n                    (beta_1 + slope_re) * session +\n                    rnorm(dplyr::n(), mean = 0, sd = sigma_error)\n  )\n\n# View a few rows\nhead(sim_data)\n\n\n# A tibble: 6 × 5\n  subject session intercept_re slope_re reaction_time\n  &lt;fct&gt;     &lt;int&gt;        &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 1             0         63.5     14.4          663.\n2 1             1         63.5     14.4          703.\n3 1             2         63.5     14.4          673.\n4 1             3         63.5     14.4          691.\n5 1             4         63.5     14.4          660.\n6 2             0        -39.0     15.7          566.\n\n\n\n\n\n\n\nCode\nstr(sim_data) # for more info, consult ?lme4::sleepstudy for the lme4 documentation\n\n\ntibble [150 × 5] (S3: tbl_df/tbl/data.frame)\n $ subject      : Factor w/ 30 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 2 2 2 2 2 ...\n $ session      : int [1:150] 0 1 2 3 4 0 1 2 3 4 ...\n $ intercept_re : num [1:150] 63.5 63.5 63.5 63.5 63.5 ...\n $ slope_re     : num [1:150] 14.4 14.4 14.4 14.4 14.4 ...\n $ reaction_time: num [1:150] 663 703 673 691 660 ...\n\n\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(y=reaction_time, x=session)) +\n  facet_wrap(~ subject, ncol=6) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(limits=c(0, 5),breaks=c(0:5))\n\n\n\n\n\n\n\n\n\n\n\nCode\nRI_only &lt;- lme4::lmer(reaction_time ~ 1 + session + (1 | subject), sim_data)\nsummary(RI_only)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reaction_time ~ 1 + session + (1 | subject)\n   Data: sim_data\n\nREML criterion at convergence: 1639.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6679 -0.4728 -0.0547  0.4959  3.8078 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n subject  (Intercept) 5402     73.50   \n Residual             2103     45.85   \nNumber of obs: 150, groups:  subject, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  614.455     14.904  41.229\nsession       -7.529      2.647  -2.844\n\nCorrelation of Fixed Effects:\n        (Intr)\nsession -0.355\n\n\nCode\n2 * pnorm(-1.744)\n\n\n[1] 0.08115909\n\n\nCode\nRI_RS_corr &lt;- lme4::lmer(reaction_time ~ 1 + session + (session | subject), sim_data)\nsummary(RI_RS_corr)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reaction_time ~ 1 + session + (session | subject)\n   Data: sim_data\n\nREML criterion at convergence: 1550.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.06073 -0.52670 -0.00823  0.53737  2.59156 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subject  (Intercept) 2090.4   45.72        \n          session      565.9   23.79    0.30\n Residual              723.6   26.90        \nNumber of obs: 150, groups:  subject, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  614.455      9.174  66.981\nsession       -7.529      4.612  -1.632\n\nCorrelation of Fixed Effects:\n        (Intr)\nsession 0.147 \n\n\nCode\n2 * pnorm(-1.428)\n\n\n[1] 0.1532919\n\n\n\n\nCode\nsleepstudy |&gt; \n  ggplot(aes(y=Reaction, x=Days)) +\n  facet_wrap(~ Subject, ncol=6) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nanova_approach &lt;- lm(Reaction ~ 1 + Days, sleepstudy)\nsummary(anova_approach)\n\n\n\nCall:\nlm(formula = Reaction ~ 1 + Days, data = sleepstudy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-110.848  -27.483    1.546   26.142  139.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  251.405      6.610  38.033  &lt; 2e-16 ***\nDays          10.467      1.238   8.454 9.89e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.71 on 178 degrees of freedom\nMultiple R-squared:  0.2865,    Adjusted R-squared:  0.2825 \nF-statistic: 71.46 on 1 and 178 DF,  p-value: 9.894e-15\n\n\nHere is the model prediction - notice how every participant is assumed to have the same initial value and linear trajectory. This is due to pooling, an assumption from ANOVA.\n\n\nCode\nggplot(sleepstudy, aes(Days, Reaction, group=Subject)) + \n  facet_wrap(~Subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(anova_approach)), linetype=2, color = \"red\") + \n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))\n\n\n\n\n\n\n\n\n\n\n\n\nIf we want to be more sensitive to individual variability, we need to allow participants to be modeled more individually. This can be done by computing linear regression for each participant, treating them as individual cases with no overlap.\n\n\nCode\nno_pooling &lt;- lmList(Reaction ~ Days | Subject, sleepstudy)\nsummary(no_pooling)\n\n\nCall:\n  Model: Reaction ~ Days | NULL \n   Data: sleepstudy \n\nCoefficients:\n   (Intercept) \n    Estimate Std. Error  t value     Pr(&gt;|t|)\n308 244.1927   15.04169 16.23439 2.419368e-34\n309 205.0549   15.04169 13.63244 1.067180e-27\n310 203.4842   15.04169 13.52802 1.993900e-27\n330 289.6851   15.04169 19.25882 1.122068e-41\n331 285.7390   15.04169 18.99647 4.646933e-41\n332 264.2516   15.04169 17.56795 1.236403e-37\n333 275.0191   15.04169 18.28379 2.303436e-39\n334 240.1629   15.04169 15.96649 1.135574e-33\n335 263.0347   15.04169 17.48705 1.946826e-37\n337 290.1041   15.04169 19.28667 9.653936e-42\n349 215.1118   15.04169 14.30104 1.983389e-29\n350 225.8346   15.04169 15.01391 2.939145e-31\n351 261.1470   15.04169 17.36155 3.943049e-37\n352 276.3721   15.04169 18.37374 1.402577e-39\n369 254.9681   15.04169 16.95077 4.023936e-36\n370 210.4491   15.04169 13.99106 1.253782e-28\n371 253.6360   15.04169 16.86221 6.656453e-36\n372 267.0448   15.04169 17.75365 4.373979e-38\n   Days \n     Estimate Std. Error    t value     Pr(&gt;|t|)\n308 21.764702   2.817566  7.7246464 1.741840e-12\n309  2.261785   2.817566  0.8027444 4.234454e-01\n310  6.114899   2.817566  2.1702769 3.162541e-02\n330  3.008073   2.817566  1.0676139 2.874813e-01\n331  5.266019   2.817566  1.8689956 6.365457e-02\n332  9.566768   2.817566  3.3954013 8.857738e-04\n333  9.142045   2.817566  3.2446604 1.462120e-03\n334 12.253141   2.817566  4.3488388 2.574673e-05\n335 -2.881034   2.817566 -1.0225257 3.082469e-01\n337 19.025974   2.817566  6.7526272 3.315759e-10\n349 13.493933   2.817566  4.7892159 4.115160e-06\n350 19.504017   2.817566  6.9222924 1.356856e-10\n351  6.433498   2.817566  2.2833528 2.387301e-02\n352 13.566549   2.817566  4.8149886 3.683105e-06\n369 11.348109   2.817566  4.0276282 9.081880e-05\n370 18.056151   2.817566  6.4084212 1.964766e-09\n371  9.188445   2.817566  3.2611283 1.385338e-03\n372 11.298073   2.817566  4.0098697 9.718197e-05\n\nResidual standard error: 25.59182 on 144 degrees of freedom\n\n\n\n\nCode\nggplot(sleepstudy, aes(Days, Reaction, group=Subject)) + \n  facet_wrap(~Subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(no_pooling)), linetype=2, color = \"red\") + \n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can relax the assumption of pooling by allowing each participant to have their own intercept.\n\n\nCode\nRI_only &lt;- lmer(Reaction ~ 1 + Days + (1 | Subject), sleepstudy)\nsummary(RI_only)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ 1 + Days + (1 | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1786.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2257 -0.5529  0.0109  0.5188  4.2506 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 1378.2   37.12   \n Residual              960.5   30.99   \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 251.4051     9.7467   25.79\nDays         10.4673     0.8042   13.02\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.371\n\n\nNote the fixed effect 10.4673 / 0.8042 = 13.0157921\n\n\nCode\nggplot(sleepstudy, aes(Days, Reaction, group=Subject)) + \n  facet_wrap(~Subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(RI_only)), linetype=2, color = \"red\") + \n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nRI_RS_corr &lt;- lmer(Reaction ~ 1 + Days + (1 + Days | Subject), sleepstudy)\nsummary(RI_RS_corr)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ 1 + Days + (1 + Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9536 -0.4634  0.0231  0.4634  5.1793 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n Subject  (Intercept) 612.10   24.741       \n          Days         35.07    5.922   0.07\n Residual             654.94   25.592       \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  251.405      6.825  36.838\nDays          10.467      1.546   6.771\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.138\n\n\nNote the fixed effect 10.467 / 1.546 = 6.7703752\n\n\nCode\nggplot(sleepstudy, aes(Days, Reaction, group=Subject, colour=Subject)) + \n  facet_wrap(~Subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(RI_RS_corr)), linetype=2, color = \"red\") + \n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(sleepstudy, aes(Days, Reaction, group=Subject)) + \n  facet_wrap(~Subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(anova_approach)), linetype=2, color = \"red\") + \n  geom_line(aes(y=fitted(no_pooling)), linetype=2, color = \"green\") + \n  geom_line(aes(y=fitted(RI_only)), linetype=2, color = \"blue\") + \n  geom_line(aes(y=fitted(RI_RS_corr)), linetype=2, color = \"black\") +\n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile random intercept models are within the class of linear mixed-effect models, they are often difficult to find use cases for within cognitive neuroscience experiments.\nWhile accouting for individual starting differences is a set up from the ANOVA approach, it still assumes no variability between participants.\nFitting mass linear models (no pooling) overcomes this problem. However, researchers often aim to generalize and make population-based conclusions. It is therefore not optimal for this use case.\nLinear mixed-effect models that are modeled appropriately to a specific data set is a great approach (partial-pooling). However, they quickly become tricky to set up when there are multiple predictors, groups, covariates, and especially interactions."
  },
  {
    "objectID": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#generating-our-data-set",
    "href": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#generating-our-data-set",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "Code\nset.seed(314)\n\n# Parameters\nn_subjects &lt;- 30\nn_sessions &lt;- 5\n\n# Fixed effect of session: general learning effect\nbeta_0 &lt;- 600   # Average baseline RT in ms\nbeta_1 &lt;- -10   # Average decrease in RT per session\n\n# Random effects: standard deviations\nsd_intercept &lt;- 50    # SD of baseline RTs across subjects\nsd_slope     &lt;- 25    # SD of learning rates (slopes)\nrho          &lt;- 0.2   # Correlation between intercepts and slopes\n\n# Within-subject residual error\nsigma_error &lt;- 30\n\n# Construct subject-level random effects (correlated)\nsubject_re &lt;- MASS::mvrnorm(\n  n = n_subjects,\n  mu = c(0, 0),\n  Sigma = matrix(c(sd_intercept^2, rho*sd_intercept*sd_slope,\n                   rho*sd_intercept*sd_slope, sd_slope^2), 2)\n) |&gt; \n  as.data.frame() |&gt;\n  setNames(c(\"intercept_re\", \"slope_re\")) |&gt;\n  dplyr::mutate(subject = factor(1:n_subjects))\n\n# Create session vector repeated for each subject\nsession &lt;- rep(0:(n_sessions - 1), times = n_subjects)\n\n# Repeat subject IDs for each session\nsubject &lt;- rep(subject_re$subject, each = n_sessions)\n\n# Join the random effects\nsim_data &lt;- tibble::tibble(subject = subject, session = session) |&gt;\n  dplyr::left_join(subject_re, by = \"subject\") |&gt;\n  dplyr::mutate(\n    reaction_time = beta_0 + intercept_re +\n                    (beta_1 + slope_re) * session +\n                    rnorm(dplyr::n(), mean = 0, sd = sigma_error)\n  )\n\n# View a few rows\nhead(sim_data)\n\n\n# A tibble: 6 × 5\n  subject session intercept_re slope_re reaction_time\n  &lt;fct&gt;     &lt;int&gt;        &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 1             0         63.5     14.4          663.\n2 1             1         63.5     14.4          703.\n3 1             2         63.5     14.4          673.\n4 1             3         63.5     14.4          691.\n5 1             4         63.5     14.4          660.\n6 2             0        -39.0     15.7          566."
  },
  {
    "objectID": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#a-brief-look-at-the-data",
    "href": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#a-brief-look-at-the-data",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "Code\nstr(sim_data) # for more info, consult ?lme4::sleepstudy for the lme4 documentation\n\n\ntibble [150 × 5] (S3: tbl_df/tbl/data.frame)\n $ subject      : Factor w/ 30 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 2 2 2 2 2 ...\n $ session      : int [1:150] 0 1 2 3 4 0 1 2 3 4 ...\n $ intercept_re : num [1:150] 63.5 63.5 63.5 63.5 63.5 ...\n $ slope_re     : num [1:150] 14.4 14.4 14.4 14.4 14.4 ...\n $ reaction_time: num [1:150] 663 703 673 691 660 ...\n\n\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(y=reaction_time, x=session)) +\n  facet_wrap(~ subject, ncol=6) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(limits=c(0, 5),breaks=c(0:5))\n\n\n\n\n\n\n\n\n\n\n\nCode\nRI_only &lt;- lme4::lmer(reaction_time ~ 1 + session + (1 | subject), sim_data)\nsummary(RI_only)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reaction_time ~ 1 + session + (1 | subject)\n   Data: sim_data\n\nREML criterion at convergence: 1639.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6679 -0.4728 -0.0547  0.4959  3.8078 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n subject  (Intercept) 5402     73.50   \n Residual             2103     45.85   \nNumber of obs: 150, groups:  subject, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  614.455     14.904  41.229\nsession       -7.529      2.647  -2.844\n\nCorrelation of Fixed Effects:\n        (Intr)\nsession -0.355\n\n\nCode\n2 * pnorm(-1.744)\n\n\n[1] 0.08115909\n\n\nCode\nRI_RS_corr &lt;- lme4::lmer(reaction_time ~ 1 + session + (session | subject), sim_data)\nsummary(RI_RS_corr)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reaction_time ~ 1 + session + (session | subject)\n   Data: sim_data\n\nREML criterion at convergence: 1550.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.06073 -0.52670 -0.00823  0.53737  2.59156 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subject  (Intercept) 2090.4   45.72        \n          session      565.9   23.79    0.30\n Residual              723.6   26.90        \nNumber of obs: 150, groups:  subject, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  614.455      9.174  66.981\nsession       -7.529      4.612  -1.632\n\nCorrelation of Fixed Effects:\n        (Intr)\nsession 0.147 \n\n\nCode\n2 * pnorm(-1.428)\n\n\n[1] 0.1532919\n\n\n\n\nCode\nsleepstudy |&gt; \n  ggplot(aes(y=Reaction, x=Days)) +\n  facet_wrap(~ Subject, ncol=6) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))"
  },
  {
    "objectID": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#the-anova-approach-full-pooling",
    "href": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#the-anova-approach-full-pooling",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "Code\nanova_approach &lt;- lm(Reaction ~ 1 + Days, sleepstudy)\nsummary(anova_approach)\n\n\n\nCall:\nlm(formula = Reaction ~ 1 + Days, data = sleepstudy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-110.848  -27.483    1.546   26.142  139.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  251.405      6.610  38.033  &lt; 2e-16 ***\nDays          10.467      1.238   8.454 9.89e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.71 on 178 degrees of freedom\nMultiple R-squared:  0.2865,    Adjusted R-squared:  0.2825 \nF-statistic: 71.46 on 1 and 178 DF,  p-value: 9.894e-15\n\n\nHere is the model prediction - notice how every participant is assumed to have the same initial value and linear trajectory. This is due to pooling, an assumption from ANOVA.\n\n\nCode\nggplot(sleepstudy, aes(Days, Reaction, group=Subject)) + \n  facet_wrap(~Subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(anova_approach)), linetype=2, color = \"red\") + \n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))"
  },
  {
    "objectID": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#no-pooling---treating-every-participant-as-their-own-experiment",
    "href": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#no-pooling---treating-every-participant-as-their-own-experiment",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "If we want to be more sensitive to individual variability, we need to allow participants to be modeled more individually. This can be done by computing linear regression for each participant, treating them as individual cases with no overlap.\n\n\nCode\nno_pooling &lt;- lmList(Reaction ~ Days | Subject, sleepstudy)\nsummary(no_pooling)\n\n\nCall:\n  Model: Reaction ~ Days | NULL \n   Data: sleepstudy \n\nCoefficients:\n   (Intercept) \n    Estimate Std. Error  t value     Pr(&gt;|t|)\n308 244.1927   15.04169 16.23439 2.419368e-34\n309 205.0549   15.04169 13.63244 1.067180e-27\n310 203.4842   15.04169 13.52802 1.993900e-27\n330 289.6851   15.04169 19.25882 1.122068e-41\n331 285.7390   15.04169 18.99647 4.646933e-41\n332 264.2516   15.04169 17.56795 1.236403e-37\n333 275.0191   15.04169 18.28379 2.303436e-39\n334 240.1629   15.04169 15.96649 1.135574e-33\n335 263.0347   15.04169 17.48705 1.946826e-37\n337 290.1041   15.04169 19.28667 9.653936e-42\n349 215.1118   15.04169 14.30104 1.983389e-29\n350 225.8346   15.04169 15.01391 2.939145e-31\n351 261.1470   15.04169 17.36155 3.943049e-37\n352 276.3721   15.04169 18.37374 1.402577e-39\n369 254.9681   15.04169 16.95077 4.023936e-36\n370 210.4491   15.04169 13.99106 1.253782e-28\n371 253.6360   15.04169 16.86221 6.656453e-36\n372 267.0448   15.04169 17.75365 4.373979e-38\n   Days \n     Estimate Std. Error    t value     Pr(&gt;|t|)\n308 21.764702   2.817566  7.7246464 1.741840e-12\n309  2.261785   2.817566  0.8027444 4.234454e-01\n310  6.114899   2.817566  2.1702769 3.162541e-02\n330  3.008073   2.817566  1.0676139 2.874813e-01\n331  5.266019   2.817566  1.8689956 6.365457e-02\n332  9.566768   2.817566  3.3954013 8.857738e-04\n333  9.142045   2.817566  3.2446604 1.462120e-03\n334 12.253141   2.817566  4.3488388 2.574673e-05\n335 -2.881034   2.817566 -1.0225257 3.082469e-01\n337 19.025974   2.817566  6.7526272 3.315759e-10\n349 13.493933   2.817566  4.7892159 4.115160e-06\n350 19.504017   2.817566  6.9222924 1.356856e-10\n351  6.433498   2.817566  2.2833528 2.387301e-02\n352 13.566549   2.817566  4.8149886 3.683105e-06\n369 11.348109   2.817566  4.0276282 9.081880e-05\n370 18.056151   2.817566  6.4084212 1.964766e-09\n371  9.188445   2.817566  3.2611283 1.385338e-03\n372 11.298073   2.817566  4.0098697 9.718197e-05\n\nResidual standard error: 25.59182 on 144 degrees of freedom\n\n\n\n\nCode\nggplot(sleepstudy, aes(Days, Reaction, group=Subject)) + \n  facet_wrap(~Subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(no_pooling)), linetype=2, color = \"red\") + \n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))"
  },
  {
    "objectID": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#partial-pooling",
    "href": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#partial-pooling",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "We can relax the assumption of pooling by allowing each participant to have their own intercept.\n\n\nCode\nRI_only &lt;- lmer(Reaction ~ 1 + Days + (1 | Subject), sleepstudy)\nsummary(RI_only)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ 1 + Days + (1 | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1786.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2257 -0.5529  0.0109  0.5188  4.2506 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 1378.2   37.12   \n Residual              960.5   30.99   \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 251.4051     9.7467   25.79\nDays         10.4673     0.8042   13.02\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.371\n\n\nNote the fixed effect 10.4673 / 0.8042 = 13.0157921\n\n\nCode\nggplot(sleepstudy, aes(Days, Reaction, group=Subject)) + \n  facet_wrap(~Subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(RI_only)), linetype=2, color = \"red\") + \n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nRI_RS_corr &lt;- lmer(Reaction ~ 1 + Days + (1 + Days | Subject), sleepstudy)\nsummary(RI_RS_corr)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ 1 + Days + (1 + Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9536 -0.4634  0.0231  0.4634  5.1793 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n Subject  (Intercept) 612.10   24.741       \n          Days         35.07    5.922   0.07\n Residual             654.94   25.592       \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  251.405      6.825  36.838\nDays          10.467      1.546   6.771\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.138\n\n\nNote the fixed effect 10.467 / 1.546 = 6.7703752\n\n\nCode\nggplot(sleepstudy, aes(Days, Reaction, group=Subject, colour=Subject)) + \n  facet_wrap(~Subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(RI_RS_corr)), linetype=2, color = \"red\") + \n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))"
  },
  {
    "objectID": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#comparing-model-predictions",
    "href": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#comparing-model-predictions",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "Code\nggplot(sleepstudy, aes(Days, Reaction, group=Subject)) + \n  facet_wrap(~Subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(anova_approach)), linetype=2, color = \"red\") + \n  geom_line(aes(y=fitted(no_pooling)), linetype=2, color = \"green\") + \n  geom_line(aes(y=fitted(RI_only)), linetype=2, color = \"blue\") + \n  geom_line(aes(y=fitted(RI_RS_corr)), linetype=2, color = \"black\") +\n  scale_x_continuous(limits=c(0, 9),breaks=c(0:9))"
  },
  {
    "objectID": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#conclusions",
    "href": "ideas_for_future_blog_posts/pooling_in_multilevel_models.html#conclusions",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "While random intercept models are within the class of linear mixed-effect models, they are often difficult to find use cases for within cognitive neuroscience experiments.\nWhile accouting for individual starting differences is a set up from the ANOVA approach, it still assumes no variability between participants.\nFitting mass linear models (no pooling) overcomes this problem. However, researchers often aim to generalize and make population-based conclusions. It is therefore not optimal for this use case.\nLinear mixed-effect models that are modeled appropriately to a specific data set is a great approach (partial-pooling). However, they quickly become tricky to set up when there are multiple predictors, groups, covariates, and especially interactions."
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "About six months ago, I came across these two memes:\n\n\n\n\n\nMeme 1\n\n\n\n\n\n\nMeme 2\n\n\n\n\nAt first, I was genuinely puzzled. What exactly is the connection between adding a random intercept in a regression model and inflating Type I error when you don’t include a random slope?\nIf you, like I did, got intrigued by these meme’s nuggets of information, consider reading along.\nMy goal here is to unpack what’s going on in a way that’s accessible, honest, and hopefully a little enjoyable. I’m not writing a textbook chapter; this is more of a personal explanation of a concept I found tricky and later came to appreciate.\n⚠️ Note: I assume you have a basic understanding of regression models and null-significance-hypothesis-tests. What follows is not a deep statistical dive, but it should give you just enough to understand the meme, avoid a common pitfall, and get curious about mixed-effects models. The material is based on:\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge university press. **A comprehensive book on the topic of *multilevel models**\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of memory and language, 68(3), 255-278. A simulation study showing the type-1 error problem of random intercept only models\nGelman, A. (2005). Analysis of variance—why it is more important than ever.\n\n\n\n\nWe’ll focus on why random slopes are crucial in many types of data—especially in biometric or health science research, where repeated measures designs are common. We’ll use a small simulation in R to illustrate the key concepts, relying on vanillalme4 and some tidyverse-styled data wrangling.\nThe data set we’ll use to demonstrate this comes from a simple simulation that mimics a common experimental design: repeated measures of a response variable (here: reaction time) across multiple sessions per participant.\nImagine we are testing whether people improve (get faster) with repeated practice. This is a typical setup in cognitive psychology. Often, researchers want to know: is there a significant learning effect over time?\nWe might want to investigate whether a test we’re performing has an undesirable shift across multiple repeated measures (stability of test instrument) in order to validate it and show it’s feasibility as a repeated measure for a clinical trial, for example for a medical assessment for driving skills under the influence of a given drug.\nBut here’s the issue (SPOILER ALLERT!): if you don’t model the data closely, you might get fundamentally different results!\nLet’s dive in and see why.\n\n\n\nWe simulate a data set where each subject completes five sessions of a task. There’s a general trend toward faster responses over time, but with subject-specific differences in both starting reaction time and learning rate.\n\n\nCode\nset.seed(314)\n\n# Parameters\nn_subjects &lt;- 30\nn_sessions &lt;- 5\n\n# Fixed effect of session: general learning effect\nbeta_0 &lt;- 600   # Average baseline RT in ms\nbeta_1 &lt;- -10   # Average decrease in RT per session\n\n# Random effects: standard deviations\nsd_intercept &lt;- 50    # SD of baseline RTs across subjects\nsd_slope     &lt;- 25    # SD of learning rates (slopes)\nrho          &lt;- 0.2   # Correlation between intercepts and slopes\n\n# Within-subject residual error\nsigma_error &lt;- 30\n\n# Construct subject-level random effects (correlated)\nsubject_re &lt;- MASS::mvrnorm(\n  n = n_subjects,\n  mu = c(0, 0),\n  Sigma = matrix(c(sd_intercept^2, rho*sd_intercept*sd_slope,\n                   rho*sd_intercept*sd_slope, sd_slope^2), 2)\n) |&gt; \n  as.data.frame() |&gt;\n  setNames(c(\"intercept_re\", \"slope_re\")) |&gt;\n  dplyr::mutate(subject = factor(1:n_subjects))\n\n# Create session vector repeated for each subject\nsession &lt;- rep(0:(n_sessions - 1), times = n_subjects)\n\n# Repeat subject IDs for each session\nsubject &lt;- rep(subject_re$subject, each = n_sessions)\n\n# Join the random effects\nsim_data &lt;- tibble::tibble(subject = subject, session = session) |&gt;\n  dplyr::left_join(subject_re, by = \"subject\") |&gt;\n  dplyr::mutate(\n    reaction_time = beta_0 + intercept_re +\n                    (beta_1 + slope_re) * session +\n                    rnorm(dplyr::n(), mean = 0, sd = sigma_error)\n  )\n\n\nHere’s the distribution of slopes (ie. learning rate variability):\n\n\nCode\nsubject_re |&gt;\n  ggplot(aes(x = slope_re)) +\n  geom_histogram(bins = 20, fill = \"#6495ED\", color = \"white\") +\n  labs(title = \"Distribution of subject-specific learning rates (slopes)\",\n       x = \"Subject slope (ms per session)\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s the structure of our data set:\n\n\nCode\ndplyr::glimpse(sim_data)\n\n\nRows: 150\nColumns: 5\n$ subject       &lt;fct&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,…\n$ session       &lt;int&gt; 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,…\n$ intercept_re  &lt;dbl&gt; 63.50593, 63.50593, 63.50593, 63.50593, 63.50593, -38.98…\n$ slope_re      &lt;dbl&gt; 14.352662, 14.352662, 14.352662, 14.352662, 14.352662, 1…\n$ reaction_time &lt;dbl&gt; 662.9042, 703.3984, 672.8058, 690.5320, 659.7003, 565.97…\n\n\nAnd each participants’ responses:\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(y=reaction_time, x=session)) +\n  facet_wrap(~ subject, ncol=6) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(limits=c(0, 4),breaks=c(0:4)) +\n  labs(x = \"Session\", y = \"Reaction Time (ms)\",\n     title = \"Individual learning trajectories across sessions\")\n\n\n\n\n\n\n\n\n\nGenerally, our sample shows the following trend at the group-level:\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(session, reaction_time)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", formula = 'y ~ x', color = \"firebrick\", se = FALSE) +\n  labs(title = \"Pooled average trend across all subjects\",\n       x = \"Session\", y = \"Reaction Time (ms)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore diving into mixed models, let’s start with a simple linear model — the kind you’d use in a basic ANOVA-style analysis. This model assumes complete pooling, meaning all subjects are treated as coming from the same population with no individual differences.\n\n\nCode\nanova_approach &lt;- lm(reaction_time ~ 1 + session, sim_data)\nsummary(anova_approach)\n\n\n\nCall:\nlm(formula = reaction_time ~ 1 + session, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-230.76  -53.64   -3.20   52.50  341.12 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  614.455     12.161  50.525   &lt;2e-16 ***\nsession       -7.529      4.965  -1.516    0.132    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 85.99 on 148 degrees of freedom\nMultiple R-squared:  0.0153,    Adjusted R-squared:  0.008646 \nF-statistic: 2.299 on 1 and 148 DF,  p-value: 0.1316\n\n\nThis model estimates a single intercept (baseline reaction time) and a single slope (learning rate) for everyone. It completely ignores the fact that each participant provides multiple data points.\n…Here is the model prediction - notice how every participant is assumed to have the same initial value and linear trajectory. This is due to pooling, an assumption from ANOVA. …\nLet’s overlay the model predictions onto each subject’s data:\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(session, reaction_time, group=subject)) + \n  facet_wrap(~subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(anova_approach)), linetype=2, color = \"red\") + \n  scale_x_continuous(limits=c(0, 4),breaks=c(0:4))\n\n\n\n\n\n\n\n\n\nAs you can see, the red dashed line is identical across all panels. That’s because the model assumes all participants behave the same — same starting point, same rate of learning. It completely ignores the fact that participants vary in their baseline speed and how much they improve. This is the statistical assumption behind traditional ANOVA: everyone is treated as identical except for random noise.\nThe elusive discussion of responders/non-responders\nAt first glance, you might look at this and say:\n“Ah, some people are improving, others aren’t — we have responders and non-responders!”\nBut that interpretation would be misleading here. This model can’t tell you who is improving — it just imposes the same trend on everyone. The differences you see are purely residuals (errors), not modeled variation.\n\n\n\n\nModeling each participant separately (no pooling)\nTo better capture individual differences, we can treat each participant as their own mini-experiment. This means fitting a separate linear regression model for each subject, without assuming any shared parameters. This is the no pooling approach — the opposite extreme of the ANOVA model.\n… If we want to be more sensitive to individual variability, we need to allow participants to be modeled more individually. This can be done by computing linear regression for each participant, treating them as individual cases with no overlap. …\nThere is this need function in lme4 that does this for you:\n\n\nCode\nno_pooling &lt;- lme4::lmList(reaction_time ~ session | subject, sim_data)\nsummary(no_pooling)\n\n\nCall:\n  Model: reaction_time ~ session | NULL \n   Data: sim_data \n\nCoefficients:\n   (Intercept) \n   Estimate Std. Error  t value     Pr(&gt;|t|)\n1  681.7230   20.83618 32.71823 9.395527e-52\n2  577.6282   20.83618 27.72237 7.396652e-46\n3  632.6230   20.83618 30.36175 4.511656e-49\n4  679.6080   20.83618 32.61672 1.216269e-51\n5  578.4442   20.83618 27.76153 6.600455e-46\n6  598.0607   20.83618 28.70299 4.435086e-47\n7  616.0432   20.83618 29.56603 3.970179e-48\n8  607.6337   20.83618 29.16243 1.218478e-47\n9  581.2502   20.83618 27.89620 4.465845e-46\n10 652.8119   20.83618 31.33069 3.399765e-50\n11 566.5336   20.83618 27.18989 3.524528e-45\n12 564.4954   20.83618 27.09208 4.707596e-45\n13 715.1396   20.83618 34.32201 1.736935e-53\n14 630.0177   20.83618 30.23671 6.329860e-49\n15 662.4253   20.83618 31.79207 1.015997e-50\n16 665.1181   20.83618 31.92130 7.262779e-51\n17 708.8651   20.83618 34.02087 3.629525e-53\n18 615.6480   20.83618 29.54707 4.183817e-48\n19 495.4300   20.83618 23.77739 1.426033e-40\n20 613.2376   20.83618 29.43138 5.763393e-48\n21 643.3719   20.83618 30.87763 1.129423e-49\n22 605.7133   20.83618 29.07026 1.576892e-47\n23 583.3266   20.83618 27.99585 3.347891e-46\n24 594.4219   20.83618 28.52835 7.279131e-47\n25 661.4904   20.83618 31.74720 1.141889e-50\n26 536.7667   20.83618 25.76128 2.624430e-43\n27 585.9676   20.83618 28.12260 2.323417e-46\n28 622.5018   20.83618 29.87600 1.692176e-48\n29 551.4993   20.83618 26.46835 3.039224e-44\n30 605.8404   20.83618 29.07637 1.550161e-47\n   session \n       Estimate Std. Error      t value     Pr(&gt;|t|)\n1   -1.92741626   8.506335 -0.226585971 8.212595e-01\n2    8.42613664   8.506335  0.990571880 3.245504e-01\n3   17.15000676   8.506335  2.016145140 4.676631e-02\n4  -19.48394859   8.506335 -2.290522028 2.432725e-02\n5   15.05841164   8.506335  1.770258395 8.007041e-02\n6   52.82327204   8.506335  6.209874124 1.604886e-08\n7  -28.98344395   8.506335 -3.407277354 9.828407e-04\n8  -27.01429459   8.506335 -3.175785264 2.046924e-03\n9  -27.21776736   8.506335 -3.199705408 1.900477e-03\n10  63.03441859   8.506335  7.410290764 6.577572e-11\n11   0.98436378   8.506335  0.115721252 9.081313e-01\n12 -32.17990897   8.506335 -3.783051982 2.786396e-04\n13   6.33994882   8.506335  0.745320814 4.580199e-01\n14 -40.56637674   8.506335 -4.768960413 7.097137e-06\n15   0.67470483   8.506335  0.079317920 9.369559e-01\n16 -17.78312994   8.506335 -2.090574745 3.938578e-02\n17  -0.29796817   8.506335 -0.035028971 9.721342e-01\n18 -40.63502607   8.506335 -4.777030789 6.873853e-06\n19 -34.88493879   8.506335 -4.101053765 9.003999e-05\n20  -8.77430658   8.506335 -1.031502542 3.050692e-01\n21  -8.86818963   8.506335 -1.042539381 2.999540e-01\n22 -14.64719423   8.506335 -1.721915908 8.852003e-02\n23 -34.43488446   8.506335 -4.048145632 1.090719e-04\n24 -28.50260901   8.506335 -3.350750670 1.179326e-03\n25 -10.96645889   8.506335 -1.289210734 2.006284e-01\n26  23.05147176   8.506335  2.709918043 8.057916e-03\n27  -0.07360777   8.506335 -0.008653288 9.931149e-01\n28 -29.04004938   8.506335 -3.413931857 9.618532e-04\n29 -17.03479502   8.506335 -2.002600913 4.822955e-02\n30   9.91250594   8.506335  1.165308618 2.469733e-01\n\nResidual standard error: 26.89939 on 90 degrees of freedom\n\n\nNow, the model fits the data as follows:\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(session, reaction_time, group=subject)) + \n  facet_wrap(~subject, ncol=6) +\n  geom_point(alpha = 0.6) + \n  geom_line(aes(y = fitted(no_pooling)), color = \"red\", linetype = \"dashed\") + \n  labs(title = \"No pooling: Separate regression for each subject\",\n       subtitle = \"Each subject gets their own intercept and slope\",\n       x = \"Days of sleep deprivation\", y = \"Reaction time (ms)\") + \n  scale_x_continuous(limits=c(0, 4),breaks=c(0:4))\n\n\n\n\n\n\n\n\n\nWhat does this show? Now, each subject has their own line, fitted independently from the others.\nThis approach fully respects subject-level variation — but at a cost.\nAlthough no pooling gives us maximum flexibility, it also comes with trade-offs:\n\nIt doesn’t borrow strength across subjects. If someone has noisy data, their slope might be wildly off.\nThere’s no generalization — you can’t talk about an average effect across subjects, only 30 separate stories.\nIn small datasets (as is common in biomedical studies), this often leads to unstable estimates.\n\nKey point No pooling shows us the raw heterogeneity in slopes and intercepts — but it doesn’t help us make population-level inferences or control for noise. We need something in between.\nWhat if we want to both model individual differences and estimate a general trend across subjects?\n\n\n\nThis is where things get interesting — we’re now ready to bridge the gap between the overly simplistic (ANOVA) and the overly fragmented (no pooling) approaches.\nPartial pooling with random intercepts\nThe random intercept model is the first step toward balancing the extremes of full pooling and no pooling.\nWe acknowledge that subjects vary in their baseline levels (reaction times), but we still assume they share a common trend — in this case, how their reaction times change over sessions.\nThe following code chunk and plot is based on a mixed-effects model: it includes both fixed effects (shared across all subjects) and random effects (varying by subject).\n\n\nCode\nRI_only &lt;- lme4::lmer(reaction_time ~ 1 + session + (1 | subject), sim_data)\nsummary(RI_only)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reaction_time ~ 1 + session + (1 | subject)\n   Data: sim_data\n\nREML criterion at convergence: 1639.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6679 -0.4728 -0.0547  0.4959  3.8078 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n subject  (Intercept) 5402     73.50   \n Residual             2103     45.85   \nNumber of obs: 150, groups:  subject, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  614.455     14.904  41.229\nsession       -7.529      2.647  -2.844\n\nCorrelation of Fixed Effects:\n        (Intr)\nsession -0.355\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(session, reaction_time, group=subject)) + \n  facet_wrap(~subject, ncol=6) +\n  geom_point(alpha = 0.6) + \n  geom_line(aes(y = fitted(RI_only)), color = \"red\", linetype = \"dashed\") + \n  labs(title = \"No pooling: Separate regression for each subject\",\n       subtitle = \"Each subject gets their own intercept and slope\",\n       x = \"Days of sleep deprivation\", y = \"Reaction time (ms)\") + \n  scale_x_continuous(limits=c(0, 4),breaks=c(0:4))\n\n\n\n\n\n\n\n\n\nWhat does this model assume?\nEach subject has their own intercept (baseline reaction time), but they all share the same learning (or fatigue) slope. This is often better than full pooling, because it accounts for subject differences in level — but it may still be too rigid if slopes really do vary. If some participants improve rapidly while others don’t, a fixed slope assumption can:\n\nUnderestimate uncertainty in the group trend,\nBias the estimate of the average slope,\nLead to false positives (thinking there’s a trend when it’s driven by a few outliers),\nMislead clinical interpretations — e.g. calling someone a “responder” when it’s just model misfit.\n\n\n\n\nIn applied biomedical and clinical research, it is common to assess whether individuals vary in their response to an intervention or over repeated measurements. A random intercept model allows for differences in baseline levels between participants but assumes that all individuals share a common rate of change (i.e., a fixed slope).\nThis assumption can be problematic. If individuals truly differ in their trajectories—such as some improving and others not—then forcing a single slope across all subjects may misattribute systematic variation to residual noise. This can result in biased fixed-effect estimates and inflated residual variance, especially if a small number of participants deviate substantially from the average trend.\nIn such cases, interpreting individual deviations from the model as evidence of “responders” or “non-responders” can be misleading. These deviations may reflect model misfit rather than genuine subject-specific effects.\nCurrent consensus in the statistical literature supports the use of random slope models when there is theoretical or empirical justification for individual differences in change over time. These models can account for both baseline heterogeneity and subject-specific trends, offering more accurate and generalizable inference (Gelman & Hill, 2007; Barr et al., 2013).\n\n\n\nTo account for individual differences not only in baseline levels but also in rates of change, we now extend the model to include random slopes. This allows each subject to have their own intercept and their own slope with respect to session.\nThis model is specified as:\n\n\nCode\nRI_RS_corr &lt;- lme4::lmer(reaction_time ~ 1 + session + (session | subject), sim_data)\nsummary(RI_RS_corr)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reaction_time ~ 1 + session + (session | subject)\n   Data: sim_data\n\nREML criterion at convergence: 1550.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.06073 -0.52670 -0.00823  0.53737  2.59156 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subject  (Intercept) 2090.4   45.72        \n          session      565.9   23.79    0.30\n Residual              723.6   26.90        \nNumber of obs: 150, groups:  subject, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  614.455      9.174  66.981\nsession       -7.529      4.612  -1.632\n\nCorrelation of Fixed Effects:\n        (Intr)\nsession 0.147 \n\n\nThis model assumes that:\n\nThe intercepts vary by subject (baseline differences),\nThe slopes also vary by subject (individual learning rates),\nAnd these random effects can be correlated (e.g., subjects with higher baselines may also learn faster or slower).\n\nBy allowing for both random intercepts and random slopes, we acknowledge that participants may respond differently over time — something particularly important in biomedical and behavioral research, where inter-individual variability is the norm rather than the exception.\nLet’s visualize the fitted lines from this model:\n\n\nCode\nsim_data$RI_RS_fitted &lt;- predict(RI_RS_corr)\n\nsim_data |&gt;\n  ggplot(aes(x = session, y = reaction_time, group = subject)) +\n  facet_wrap(~ subject, ncol = 6) +\n  geom_point(alpha = 0.6) +\n  geom_line(aes(y = RI_RS_fitted), color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Random intercept and random slope model\",\n       subtitle = \"Each subject has their own intercept and slope\",\n       x = \"Session\", y = \"Reaction Time (ms)\") +\n  scale_x_continuous(limits = c(0, 4), breaks = 0:4) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nRandom slope models can:\n\nImprove model fit by accounting for structured variability,\nProduce more accurate estimates of population-level effects,\nReduce the risk of false positives or negatives due to mis-specified structure,\nAnd provide a framework for investigating individual differences in trajectories — a key concern in many health-related studies.\n\nThis approach represents what is often called partial pooling: estimates are informed both by the individual subject’s data and by the group-level trend, leading to more stable and interpretable inference, especially in small to moderate samples.\n\n\n\nWe now compare two models:\n\nOne that allows subjects to differ only in their baseline level (random intercept),\nAnd another that also allows them to differ in their rate of change over sessions (random slope).\n\nUsing the Akaike Information Criterion, we can assess model fit:\n\n\nCode\ntibble::tibble(\n  model = c(\"ANOVA\",\"mass_regression\",\"RI_only\",\"RI_RS_corr\"),\n  AIC = c(AIC(anova_approach),AIC(no_pooling),AIC(RI_only),AIC(RI_RS_corr))\n)\n\n\n# A tibble: 4 × 2\n  model             AIC\n  &lt;chr&gt;           &lt;dbl&gt;\n1 ANOVA           1766.\n2 mass_regression 1459.\n3 RI_only         1647.\n4 RI_RS_corr      1563.\n\n\nUsing the REML criterion (restricted maximum likelihood), we can assess model fit:\n\n\n\nModel\nModel REML criterion\n\n\n\n\nRandom intercept only\n1639.3\n\n\nRandom intercept + random slope\n1550.6\n\n\n\n\n\n\n\nLower AIC values indicate better fit. Here, the random slope model shows a substantially lower AIC, suggesting that accounting for individual variability in slopes improves model fit.\nIf we wanted to formally test whether adding random slopes improves the model, we could refit both models using maximum likelihood (ML) and then conduct a likelihood ratio test:\n\n\nCode\nRI_only_ml &lt;- lmer(reaction_time ~ session + (1 | subject), sim_data, REML = FALSE)\nRI_RS_corr_ml &lt;- lmer(reaction_time ~ session + (session | subject), sim_data, REML = FALSE)\n\nanova(RI_only_ml, RI_RS_corr_ml)\n\n\nData: sim_data\nModels:\nRI_only_ml: reaction_time ~ session + (1 | subject)\nRI_RS_corr_ml: reaction_time ~ session + (session | subject)\n              npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)    \nRI_only_ml       4 1658.1 1670.2 -825.07    1650.1                         \nRI_RS_corr_ml    6 1573.7 1591.8 -780.85    1561.7 88.428  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis test examines whether the added complexity (the subject-specific slopes and the correlation between random intercept and random slope) is justified by a significantly better fit.\nThe model with random slopes:\n\nFits the data substantially better (lower REML),\nProduces more conservative fixed-effect estimates (larger SE, smaller t),\nAccounts for the correlation between intercepts and slopes across individuals (here: +0.30),\nAnd reduces residual variance (from ~2103 to ~723), indicating that more of the variability is now captured by structured random effects.\n\nThis supports the idea that in biomedical or behavioral data — where individual responses often differ — a random intercept-only model may be too restrictive and lead to misleading fixed-effect conclusions.\n\n\n\nThe random intercept + slope model with added correlation term gives us valuable insight into the structure of variability in our data — not just whether an effect exists on average, but how consistent it is across individuals.\nFrom the model output:\n\n\nCode\nsummary(RI_RS_corr)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reaction_time ~ 1 + session + (session | subject)\n   Data: sim_data\n\nREML criterion at convergence: 1550.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.06073 -0.52670 -0.00823  0.53737  2.59156 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subject  (Intercept) 2090.4   45.72        \n          session      565.9   23.79    0.30\n Residual              723.6   26.90        \nNumber of obs: 150, groups:  subject, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  614.455      9.174  66.981\nsession       -7.529      4.612  -1.632\n\nCorrelation of Fixed Effects:\n        (Intr)\nsession 0.147 \n\n\n\nBaseline (Intercept) Variability The standard deviation of the intercepts is 45.72 ms, indicating substantial between-subject differences in baseline reaction times. This is expected in behavioral or clinical data, where individuals differ due to prior experience, cognitive ability, or other latent traits.\nSlope (Session Effect) Variability The standard deviation of the random slopes is 23.79 ms, meaning that individuals differ notably in how their reaction times change over sessions — some improve sharply, others remain flat or even worsen slightly. This heterogeneity is clinically relevant and would be obscured by models that assume a common slope.\nCorrelation Between Intercepts and Slopes The correlation between intercepts and slopes is +0.30. This suggests a weak-to-moderate tendency for participants with higher baseline reaction times to show steeper improvements (more negative slopes). While not strong, this association may point to individual differences in learning potential — e.g., those starting slower have more room to improve.\n\nThis can be visualized:\n\n\nCode\nsim_data |&gt; \n  dplyr::mutate(improving = slope_re &lt; 0) |&gt; \n  ggplot(aes(y=reaction_time, x=session, color = improving)) +\n    scale_color_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"gray\")) +\n    facet_wrap(~ subject, ncol=6) +\n    geom_point() +\n    geom_line() +\n    geom_hline(yintercept= mean(subset(sim_data,session==1)$reaction_time)) +\n    scale_x_continuous(limits=c(0, 4),breaks=c(0:4)) +\n    labs(x = \"Session\", y = \"Reaction Time (ms)\",\n         title = \"Individual learning trajectories across sessions\")\n\n\n\n\n\n\n\n\n\nWhy this matters\nThe comparison to a simpler model (with only random intercepts) showed that assuming a shared slope across participants can lead to misleading fixed-effect estimates. Notice that while the estimate is identical, the standard error of the estimate increased almost two-fold, yielding a lower test statistic.\n\n\n\nModel\nFixed Effect\nEstimate\nStd. Error\nt value\n\n\n\n\nRI_only\nsession\n-7.53\n2.65\n-2.84\n\n\nRI_RS_corr\nsession\n-7.53\n4.61\n-1.63\n\n\n\nThe estimate is identical in both models, but the standard error increases substantially in the random slope model. This reflects how the RI-only model underestimates uncertainty by ignoring individual variability in slopes. Consequently, the t-value drops from a statistically suggestive level (–2.84) to a more cautious level (–1.63), demonstrating the importance of correctly specifying random effects.\nThe average effect remains the same, but the uncertainty increases, because the model properly attributes some of the variation to individual differences.\nThis is exactly what Gelman & Hill (2007) and Barr et al. (2013) argue: failing to include justified random slopes can inflate confidence in fixed effects — potentially leading to spurious conclusions.\nKey points:\nBy modeling random slopes: - We acknowledge that people learn differently over time, - We prevent overconfident or misleading group-level inferences, - And we gain access to rich insights about how variability in individual responses relates to baseline ability.\nThis modeling approach helps move us from averages to individuals, which is crucial in many health, cognitive, and behavioral domains.\n\n\n\nSo, what did we learn from all this?\nWe explored a progression of modeling approaches:\n\nFrom ANOVA-style pooling,\nto mass regression (no pooling),\nthen to partial pooling with a random intercept,\nand finally to a model with both random intercepts and slopes, including their correlation.\n\nEach step improved the model fit. But here’s the key insight: Only the model with random slopes gave us a trustworthy picture of individual differences.\nThe random intercept-only model, while tempting in its simplicity, produced misleading results. Specifically, it underestimated standard errors, leading to inflated Type I error rates. That’s not just a technical detail — it can fundamentally distort scientific conclusions, especially in clinical or psychological studies where understanding who improves and who doesn’t is crucial.\nSo, if you’re working with repeated measures or hierarchical data and you care about individual variability, responder/non-responder patterns, or generalization, don’t stop at random intercepts.\nModeling the variance-covariance structure of the specific data set (i.e. random terms of the mixed-effect model) aren’t just a technical flourish — they’re essential for honest modeling.\n\nWhile random intercept models are within the class of linear mixed-effect models, they are often difficult to find use cases for within cognitive neuroscience experiments.\nWhile accounting for individual starting differences is a step up from the ANOVA approach, it still assumes no variability between participants.\nFitting mass linear models (no pooling) overcomes this problem. However, researchers often aim to generalize and make population-based conclusions. It is therefore not optimal for this use case.\nLinear mixed-effect models that are modeled appropriately to a specific data set is a great approach (partial-pooling). However, they quickly become tricky to set up when there are multiple predictors, groups, covariates, and especially interactions."
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#how-two-memes-helped-me-learn-something-about-mixed-effect-models",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#how-two-memes-helped-me-learn-something-about-mixed-effect-models",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "About six months ago, I came across these two memes:\n\n\n\n\n\nMeme 1\n\n\n\n\n\n\nMeme 2\n\n\n\n\nAt first, I was genuinely puzzled. What exactly is the connection between adding a random intercept in a regression model and inflating Type I error when you don’t include a random slope?\nIf you, like I did, got intrigued by these meme’s nuggets of information, consider reading along.\nMy goal here is to unpack what’s going on in a way that’s accessible, honest, and hopefully a little enjoyable. I’m not writing a textbook chapter; this is more of a personal explanation of a concept I found tricky and later came to appreciate.\n⚠️ Note: I assume you have a basic understanding of regression models and null-significance-hypothesis-tests. What follows is not a deep statistical dive, but it should give you just enough to understand the meme, avoid a common pitfall, and get curious about mixed-effects models. The material is based on:\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge university press. **A comprehensive book on the topic of *multilevel models**\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of memory and language, 68(3), 255-278. A simulation study showing the type-1 error problem of random intercept only models\nGelman, A. (2005). Analysis of variance—why it is more important than ever."
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#how-well-approach-it",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#how-well-approach-it",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "We’ll focus on why random slopes are crucial in many types of data—especially in biometric or health science research, where repeated measures designs are common. We’ll use a small simulation in R to illustrate the key concepts, relying on vanillalme4 and some tidyverse-styled data wrangling.\nThe data set we’ll use to demonstrate this comes from a simple simulation that mimics a common experimental design: repeated measures of a response variable (here: reaction time) across multiple sessions per participant.\nImagine we are testing whether people improve (get faster) with repeated practice. This is a typical setup in cognitive psychology. Often, researchers want to know: is there a significant learning effect over time?\nWe might want to investigate whether a test we’re performing has an undesirable shift across multiple repeated measures (stability of test instrument) in order to validate it and show it’s feasibility as a repeated measure for a clinical trial, for example for a medical assessment for driving skills under the influence of a given drug.\nBut here’s the issue (SPOILER ALLERT!): if you don’t model the data closely, you might get fundamentally different results!\nLet’s dive in and see why."
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#simulating-our-data-set",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#simulating-our-data-set",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "We simulate a data set where each subject completes five sessions of a task. There’s a general trend toward faster responses over time, but with subject-specific differences in both starting reaction time and learning rate.\n\n\nCode\nset.seed(314)\n\n# Parameters\nn_subjects &lt;- 30\nn_sessions &lt;- 5\n\n# Fixed effect of session: general learning effect\nbeta_0 &lt;- 600   # Average baseline RT in ms\nbeta_1 &lt;- -10   # Average decrease in RT per session\n\n# Random effects: standard deviations\nsd_intercept &lt;- 50    # SD of baseline RTs across subjects\nsd_slope     &lt;- 25    # SD of learning rates (slopes)\nrho          &lt;- 0.2   # Correlation between intercepts and slopes\n\n# Within-subject residual error\nsigma_error &lt;- 30\n\n# Construct subject-level random effects (correlated)\nsubject_re &lt;- MASS::mvrnorm(\n  n = n_subjects,\n  mu = c(0, 0),\n  Sigma = matrix(c(sd_intercept^2, rho*sd_intercept*sd_slope,\n                   rho*sd_intercept*sd_slope, sd_slope^2), 2)\n) |&gt; \n  as.data.frame() |&gt;\n  setNames(c(\"intercept_re\", \"slope_re\")) |&gt;\n  dplyr::mutate(subject = factor(1:n_subjects))\n\n# Create session vector repeated for each subject\nsession &lt;- rep(0:(n_sessions - 1), times = n_subjects)\n\n# Repeat subject IDs for each session\nsubject &lt;- rep(subject_re$subject, each = n_sessions)\n\n# Join the random effects\nsim_data &lt;- tibble::tibble(subject = subject, session = session) |&gt;\n  dplyr::left_join(subject_re, by = \"subject\") |&gt;\n  dplyr::mutate(\n    reaction_time = beta_0 + intercept_re +\n                    (beta_1 + slope_re) * session +\n                    rnorm(dplyr::n(), mean = 0, sd = sigma_error)\n  )\n\n\nHere’s the distribution of slopes (ie. learning rate variability):\n\n\nCode\nsubject_re |&gt;\n  ggplot(aes(x = slope_re)) +\n  geom_histogram(bins = 20, fill = \"#6495ED\", color = \"white\") +\n  labs(title = \"Distribution of subject-specific learning rates (slopes)\",\n       x = \"Subject slope (ms per session)\", y = \"Count\") +\n  theme_minimal()"
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#a-brief-look-at-the-data",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#a-brief-look-at-the-data",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "Here’s the structure of our data set:\n\n\nCode\ndplyr::glimpse(sim_data)\n\n\nRows: 150\nColumns: 5\n$ subject       &lt;fct&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,…\n$ session       &lt;int&gt; 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,…\n$ intercept_re  &lt;dbl&gt; 63.50593, 63.50593, 63.50593, 63.50593, 63.50593, -38.98…\n$ slope_re      &lt;dbl&gt; 14.352662, 14.352662, 14.352662, 14.352662, 14.352662, 1…\n$ reaction_time &lt;dbl&gt; 662.9042, 703.3984, 672.8058, 690.5320, 659.7003, 565.97…\n\n\nAnd each participants’ responses:\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(y=reaction_time, x=session)) +\n  facet_wrap(~ subject, ncol=6) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(limits=c(0, 4),breaks=c(0:4)) +\n  labs(x = \"Session\", y = \"Reaction Time (ms)\",\n     title = \"Individual learning trajectories across sessions\")\n\n\n\n\n\n\n\n\n\nGenerally, our sample shows the following trend at the group-level:\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(session, reaction_time)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", formula = 'y ~ x', color = \"firebrick\", se = FALSE) +\n  labs(title = \"Pooled average trend across all subjects\",\n       x = \"Session\", y = \"Reaction Time (ms)\")"
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#the-anova-approach-full-pooling",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#the-anova-approach-full-pooling",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "Before diving into mixed models, let’s start with a simple linear model — the kind you’d use in a basic ANOVA-style analysis. This model assumes complete pooling, meaning all subjects are treated as coming from the same population with no individual differences.\n\n\nCode\nanova_approach &lt;- lm(reaction_time ~ 1 + session, sim_data)\nsummary(anova_approach)\n\n\n\nCall:\nlm(formula = reaction_time ~ 1 + session, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-230.76  -53.64   -3.20   52.50  341.12 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  614.455     12.161  50.525   &lt;2e-16 ***\nsession       -7.529      4.965  -1.516    0.132    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 85.99 on 148 degrees of freedom\nMultiple R-squared:  0.0153,    Adjusted R-squared:  0.008646 \nF-statistic: 2.299 on 1 and 148 DF,  p-value: 0.1316\n\n\nThis model estimates a single intercept (baseline reaction time) and a single slope (learning rate) for everyone. It completely ignores the fact that each participant provides multiple data points.\n…Here is the model prediction - notice how every participant is assumed to have the same initial value and linear trajectory. This is due to pooling, an assumption from ANOVA. …\nLet’s overlay the model predictions onto each subject’s data:\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(session, reaction_time, group=subject)) + \n  facet_wrap(~subject, ncol=6) +\n  geom_point() + \n  geom_line(aes(y=fitted(anova_approach)), linetype=2, color = \"red\") + \n  scale_x_continuous(limits=c(0, 4),breaks=c(0:4))\n\n\n\n\n\n\n\n\n\nAs you can see, the red dashed line is identical across all panels. That’s because the model assumes all participants behave the same — same starting point, same rate of learning. It completely ignores the fact that participants vary in their baseline speed and how much they improve. This is the statistical assumption behind traditional ANOVA: everyone is treated as identical except for random noise.\nThe elusive discussion of responders/non-responders\nAt first glance, you might look at this and say:\n“Ah, some people are improving, others aren’t — we have responders and non-responders!”\nBut that interpretation would be misleading here. This model can’t tell you who is improving — it just imposes the same trend on everyone. The differences you see are purely residuals (errors), not modeled variation."
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#modeling-variation-by-treating-every-participant-as-their-own-experiment-no-pooling",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#modeling-variation-by-treating-every-participant-as-their-own-experiment-no-pooling",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "Modeling each participant separately (no pooling)\nTo better capture individual differences, we can treat each participant as their own mini-experiment. This means fitting a separate linear regression model for each subject, without assuming any shared parameters. This is the no pooling approach — the opposite extreme of the ANOVA model.\n… If we want to be more sensitive to individual variability, we need to allow participants to be modeled more individually. This can be done by computing linear regression for each participant, treating them as individual cases with no overlap. …\nThere is this need function in lme4 that does this for you:\n\n\nCode\nno_pooling &lt;- lme4::lmList(reaction_time ~ session | subject, sim_data)\nsummary(no_pooling)\n\n\nCall:\n  Model: reaction_time ~ session | NULL \n   Data: sim_data \n\nCoefficients:\n   (Intercept) \n   Estimate Std. Error  t value     Pr(&gt;|t|)\n1  681.7230   20.83618 32.71823 9.395527e-52\n2  577.6282   20.83618 27.72237 7.396652e-46\n3  632.6230   20.83618 30.36175 4.511656e-49\n4  679.6080   20.83618 32.61672 1.216269e-51\n5  578.4442   20.83618 27.76153 6.600455e-46\n6  598.0607   20.83618 28.70299 4.435086e-47\n7  616.0432   20.83618 29.56603 3.970179e-48\n8  607.6337   20.83618 29.16243 1.218478e-47\n9  581.2502   20.83618 27.89620 4.465845e-46\n10 652.8119   20.83618 31.33069 3.399765e-50\n11 566.5336   20.83618 27.18989 3.524528e-45\n12 564.4954   20.83618 27.09208 4.707596e-45\n13 715.1396   20.83618 34.32201 1.736935e-53\n14 630.0177   20.83618 30.23671 6.329860e-49\n15 662.4253   20.83618 31.79207 1.015997e-50\n16 665.1181   20.83618 31.92130 7.262779e-51\n17 708.8651   20.83618 34.02087 3.629525e-53\n18 615.6480   20.83618 29.54707 4.183817e-48\n19 495.4300   20.83618 23.77739 1.426033e-40\n20 613.2376   20.83618 29.43138 5.763393e-48\n21 643.3719   20.83618 30.87763 1.129423e-49\n22 605.7133   20.83618 29.07026 1.576892e-47\n23 583.3266   20.83618 27.99585 3.347891e-46\n24 594.4219   20.83618 28.52835 7.279131e-47\n25 661.4904   20.83618 31.74720 1.141889e-50\n26 536.7667   20.83618 25.76128 2.624430e-43\n27 585.9676   20.83618 28.12260 2.323417e-46\n28 622.5018   20.83618 29.87600 1.692176e-48\n29 551.4993   20.83618 26.46835 3.039224e-44\n30 605.8404   20.83618 29.07637 1.550161e-47\n   session \n       Estimate Std. Error      t value     Pr(&gt;|t|)\n1   -1.92741626   8.506335 -0.226585971 8.212595e-01\n2    8.42613664   8.506335  0.990571880 3.245504e-01\n3   17.15000676   8.506335  2.016145140 4.676631e-02\n4  -19.48394859   8.506335 -2.290522028 2.432725e-02\n5   15.05841164   8.506335  1.770258395 8.007041e-02\n6   52.82327204   8.506335  6.209874124 1.604886e-08\n7  -28.98344395   8.506335 -3.407277354 9.828407e-04\n8  -27.01429459   8.506335 -3.175785264 2.046924e-03\n9  -27.21776736   8.506335 -3.199705408 1.900477e-03\n10  63.03441859   8.506335  7.410290764 6.577572e-11\n11   0.98436378   8.506335  0.115721252 9.081313e-01\n12 -32.17990897   8.506335 -3.783051982 2.786396e-04\n13   6.33994882   8.506335  0.745320814 4.580199e-01\n14 -40.56637674   8.506335 -4.768960413 7.097137e-06\n15   0.67470483   8.506335  0.079317920 9.369559e-01\n16 -17.78312994   8.506335 -2.090574745 3.938578e-02\n17  -0.29796817   8.506335 -0.035028971 9.721342e-01\n18 -40.63502607   8.506335 -4.777030789 6.873853e-06\n19 -34.88493879   8.506335 -4.101053765 9.003999e-05\n20  -8.77430658   8.506335 -1.031502542 3.050692e-01\n21  -8.86818963   8.506335 -1.042539381 2.999540e-01\n22 -14.64719423   8.506335 -1.721915908 8.852003e-02\n23 -34.43488446   8.506335 -4.048145632 1.090719e-04\n24 -28.50260901   8.506335 -3.350750670 1.179326e-03\n25 -10.96645889   8.506335 -1.289210734 2.006284e-01\n26  23.05147176   8.506335  2.709918043 8.057916e-03\n27  -0.07360777   8.506335 -0.008653288 9.931149e-01\n28 -29.04004938   8.506335 -3.413931857 9.618532e-04\n29 -17.03479502   8.506335 -2.002600913 4.822955e-02\n30   9.91250594   8.506335  1.165308618 2.469733e-01\n\nResidual standard error: 26.89939 on 90 degrees of freedom\n\n\nNow, the model fits the data as follows:\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(session, reaction_time, group=subject)) + \n  facet_wrap(~subject, ncol=6) +\n  geom_point(alpha = 0.6) + \n  geom_line(aes(y = fitted(no_pooling)), color = \"red\", linetype = \"dashed\") + \n  labs(title = \"No pooling: Separate regression for each subject\",\n       subtitle = \"Each subject gets their own intercept and slope\",\n       x = \"Days of sleep deprivation\", y = \"Reaction time (ms)\") + \n  scale_x_continuous(limits=c(0, 4),breaks=c(0:4))\n\n\n\n\n\n\n\n\n\nWhat does this show? Now, each subject has their own line, fitted independently from the others.\nThis approach fully respects subject-level variation — but at a cost.\nAlthough no pooling gives us maximum flexibility, it also comes with trade-offs:\n\nIt doesn’t borrow strength across subjects. If someone has noisy data, their slope might be wildly off.\nThere’s no generalization — you can’t talk about an average effect across subjects, only 30 separate stories.\nIn small datasets (as is common in biomedical studies), this often leads to unstable estimates.\n\nKey point No pooling shows us the raw heterogeneity in slopes and intercepts — but it doesn’t help us make population-level inferences or control for noise. We need something in between.\nWhat if we want to both model individual differences and estimate a general trend across subjects?"
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#multilevel-regression-partial-pooling",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#multilevel-regression-partial-pooling",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "This is where things get interesting — we’re now ready to bridge the gap between the overly simplistic (ANOVA) and the overly fragmented (no pooling) approaches.\nPartial pooling with random intercepts\nThe random intercept model is the first step toward balancing the extremes of full pooling and no pooling.\nWe acknowledge that subjects vary in their baseline levels (reaction times), but we still assume they share a common trend — in this case, how their reaction times change over sessions.\nThe following code chunk and plot is based on a mixed-effects model: it includes both fixed effects (shared across all subjects) and random effects (varying by subject).\n\n\nCode\nRI_only &lt;- lme4::lmer(reaction_time ~ 1 + session + (1 | subject), sim_data)\nsummary(RI_only)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reaction_time ~ 1 + session + (1 | subject)\n   Data: sim_data\n\nREML criterion at convergence: 1639.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6679 -0.4728 -0.0547  0.4959  3.8078 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n subject  (Intercept) 5402     73.50   \n Residual             2103     45.85   \nNumber of obs: 150, groups:  subject, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  614.455     14.904  41.229\nsession       -7.529      2.647  -2.844\n\nCorrelation of Fixed Effects:\n        (Intr)\nsession -0.355\n\n\nCode\nsim_data |&gt; \n  ggplot(aes(session, reaction_time, group=subject)) + \n  facet_wrap(~subject, ncol=6) +\n  geom_point(alpha = 0.6) + \n  geom_line(aes(y = fitted(RI_only)), color = \"red\", linetype = \"dashed\") + \n  labs(title = \"No pooling: Separate regression for each subject\",\n       subtitle = \"Each subject gets their own intercept and slope\",\n       x = \"Days of sleep deprivation\", y = \"Reaction time (ms)\") + \n  scale_x_continuous(limits=c(0, 4),breaks=c(0:4))\n\n\n\n\n\n\n\n\n\nWhat does this model assume?\nEach subject has their own intercept (baseline reaction time), but they all share the same learning (or fatigue) slope. This is often better than full pooling, because it accounts for subject differences in level — but it may still be too rigid if slopes really do vary. If some participants improve rapidly while others don’t, a fixed slope assumption can:\n\nUnderestimate uncertainty in the group trend,\nBias the estimate of the average slope,\nLead to false positives (thinking there’s a trend when it’s driven by a few outliers),\nMislead clinical interpretations — e.g. calling someone a “responder” when it’s just model misfit."
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#limitations-of-random-intercept-models-in-clinical-inference",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#limitations-of-random-intercept-models-in-clinical-inference",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "In applied biomedical and clinical research, it is common to assess whether individuals vary in their response to an intervention or over repeated measurements. A random intercept model allows for differences in baseline levels between participants but assumes that all individuals share a common rate of change (i.e., a fixed slope).\nThis assumption can be problematic. If individuals truly differ in their trajectories—such as some improving and others not—then forcing a single slope across all subjects may misattribute systematic variation to residual noise. This can result in biased fixed-effect estimates and inflated residual variance, especially if a small number of participants deviate substantially from the average trend.\nIn such cases, interpreting individual deviations from the model as evidence of “responders” or “non-responders” can be misleading. These deviations may reflect model misfit rather than genuine subject-specific effects.\nCurrent consensus in the statistical literature supports the use of random slope models when there is theoretical or empirical justification for individual differences in change over time. These models can account for both baseline heterogeneity and subject-specific trends, offering more accurate and generalizable inference (Gelman & Hill, 2007; Barr et al., 2013)."
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#random-slopes",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#random-slopes",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "To account for individual differences not only in baseline levels but also in rates of change, we now extend the model to include random slopes. This allows each subject to have their own intercept and their own slope with respect to session.\nThis model is specified as:\n\n\nCode\nRI_RS_corr &lt;- lme4::lmer(reaction_time ~ 1 + session + (session | subject), sim_data)\nsummary(RI_RS_corr)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reaction_time ~ 1 + session + (session | subject)\n   Data: sim_data\n\nREML criterion at convergence: 1550.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.06073 -0.52670 -0.00823  0.53737  2.59156 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subject  (Intercept) 2090.4   45.72        \n          session      565.9   23.79    0.30\n Residual              723.6   26.90        \nNumber of obs: 150, groups:  subject, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  614.455      9.174  66.981\nsession       -7.529      4.612  -1.632\n\nCorrelation of Fixed Effects:\n        (Intr)\nsession 0.147 \n\n\nThis model assumes that:\n\nThe intercepts vary by subject (baseline differences),\nThe slopes also vary by subject (individual learning rates),\nAnd these random effects can be correlated (e.g., subjects with higher baselines may also learn faster or slower).\n\nBy allowing for both random intercepts and random slopes, we acknowledge that participants may respond differently over time — something particularly important in biomedical and behavioral research, where inter-individual variability is the norm rather than the exception.\nLet’s visualize the fitted lines from this model:\n\n\nCode\nsim_data$RI_RS_fitted &lt;- predict(RI_RS_corr)\n\nsim_data |&gt;\n  ggplot(aes(x = session, y = reaction_time, group = subject)) +\n  facet_wrap(~ subject, ncol = 6) +\n  geom_point(alpha = 0.6) +\n  geom_line(aes(y = RI_RS_fitted), color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Random intercept and random slope model\",\n       subtitle = \"Each subject has their own intercept and slope\",\n       x = \"Session\", y = \"Reaction Time (ms)\") +\n  scale_x_continuous(limits = c(0, 4), breaks = 0:4) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nRandom slope models can:\n\nImprove model fit by accounting for structured variability,\nProduce more accurate estimates of population-level effects,\nReduce the risk of false positives or negatives due to mis-specified structure,\nAnd provide a framework for investigating individual differences in trajectories — a key concern in many health-related studies.\n\nThis approach represents what is often called partial pooling: estimates are informed both by the individual subject’s data and by the group-level trend, leading to more stable and interpretable inference, especially in small to moderate samples."
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#comparing-models-do-random-slopes-improve-fit",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#comparing-models-do-random-slopes-improve-fit",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "We now compare two models:\n\nOne that allows subjects to differ only in their baseline level (random intercept),\nAnd another that also allows them to differ in their rate of change over sessions (random slope).\n\nUsing the Akaike Information Criterion, we can assess model fit:\n\n\nCode\ntibble::tibble(\n  model = c(\"ANOVA\",\"mass_regression\",\"RI_only\",\"RI_RS_corr\"),\n  AIC = c(AIC(anova_approach),AIC(no_pooling),AIC(RI_only),AIC(RI_RS_corr))\n)\n\n\n# A tibble: 4 × 2\n  model             AIC\n  &lt;chr&gt;           &lt;dbl&gt;\n1 ANOVA           1766.\n2 mass_regression 1459.\n3 RI_only         1647.\n4 RI_RS_corr      1563.\n\n\nUsing the REML criterion (restricted maximum likelihood), we can assess model fit:\n\n\n\nModel\nModel REML criterion\n\n\n\n\nRandom intercept only\n1639.3\n\n\nRandom intercept + random slope\n1550.6\n\n\n\n\n\n\n\nLower AIC values indicate better fit. Here, the random slope model shows a substantially lower AIC, suggesting that accounting for individual variability in slopes improves model fit.\nIf we wanted to formally test whether adding random slopes improves the model, we could refit both models using maximum likelihood (ML) and then conduct a likelihood ratio test:\n\n\nCode\nRI_only_ml &lt;- lmer(reaction_time ~ session + (1 | subject), sim_data, REML = FALSE)\nRI_RS_corr_ml &lt;- lmer(reaction_time ~ session + (session | subject), sim_data, REML = FALSE)\n\nanova(RI_only_ml, RI_RS_corr_ml)\n\n\nData: sim_data\nModels:\nRI_only_ml: reaction_time ~ session + (1 | subject)\nRI_RS_corr_ml: reaction_time ~ session + (session | subject)\n              npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)    \nRI_only_ml       4 1658.1 1670.2 -825.07    1650.1                         \nRI_RS_corr_ml    6 1573.7 1591.8 -780.85    1561.7 88.428  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis test examines whether the added complexity (the subject-specific slopes and the correlation between random intercept and random slope) is justified by a significantly better fit.\nThe model with random slopes:\n\nFits the data substantially better (lower REML),\nProduces more conservative fixed-effect estimates (larger SE, smaller t),\nAccounts for the correlation between intercepts and slopes across individuals (here: +0.30),\nAnd reduces residual variance (from ~2103 to ~723), indicating that more of the variability is now captured by structured random effects.\n\nThis supports the idea that in biomedical or behavioral data — where individual responses often differ — a random intercept-only model may be too restrictive and lead to misleading fixed-effect conclusions."
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#interpreting-the-random-effects",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#interpreting-the-random-effects",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "The random intercept + slope model with added correlation term gives us valuable insight into the structure of variability in our data — not just whether an effect exists on average, but how consistent it is across individuals.\nFrom the model output:\n\n\nCode\nsummary(RI_RS_corr)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reaction_time ~ 1 + session + (session | subject)\n   Data: sim_data\n\nREML criterion at convergence: 1550.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.06073 -0.52670 -0.00823  0.53737  2.59156 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subject  (Intercept) 2090.4   45.72        \n          session      565.9   23.79    0.30\n Residual              723.6   26.90        \nNumber of obs: 150, groups:  subject, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  614.455      9.174  66.981\nsession       -7.529      4.612  -1.632\n\nCorrelation of Fixed Effects:\n        (Intr)\nsession 0.147 \n\n\n\nBaseline (Intercept) Variability The standard deviation of the intercepts is 45.72 ms, indicating substantial between-subject differences in baseline reaction times. This is expected in behavioral or clinical data, where individuals differ due to prior experience, cognitive ability, or other latent traits.\nSlope (Session Effect) Variability The standard deviation of the random slopes is 23.79 ms, meaning that individuals differ notably in how their reaction times change over sessions — some improve sharply, others remain flat or even worsen slightly. This heterogeneity is clinically relevant and would be obscured by models that assume a common slope.\nCorrelation Between Intercepts and Slopes The correlation between intercepts and slopes is +0.30. This suggests a weak-to-moderate tendency for participants with higher baseline reaction times to show steeper improvements (more negative slopes). While not strong, this association may point to individual differences in learning potential — e.g., those starting slower have more room to improve.\n\nThis can be visualized:\n\n\nCode\nsim_data |&gt; \n  dplyr::mutate(improving = slope_re &lt; 0) |&gt; \n  ggplot(aes(y=reaction_time, x=session, color = improving)) +\n    scale_color_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"gray\")) +\n    facet_wrap(~ subject, ncol=6) +\n    geom_point() +\n    geom_line() +\n    geom_hline(yintercept= mean(subset(sim_data,session==1)$reaction_time)) +\n    scale_x_continuous(limits=c(0, 4),breaks=c(0:4)) +\n    labs(x = \"Session\", y = \"Reaction Time (ms)\",\n         title = \"Individual learning trajectories across sessions\")\n\n\n\n\n\n\n\n\n\nWhy this matters\nThe comparison to a simpler model (with only random intercepts) showed that assuming a shared slope across participants can lead to misleading fixed-effect estimates. Notice that while the estimate is identical, the standard error of the estimate increased almost two-fold, yielding a lower test statistic.\n\n\n\nModel\nFixed Effect\nEstimate\nStd. Error\nt value\n\n\n\n\nRI_only\nsession\n-7.53\n2.65\n-2.84\n\n\nRI_RS_corr\nsession\n-7.53\n4.61\n-1.63\n\n\n\nThe estimate is identical in both models, but the standard error increases substantially in the random slope model. This reflects how the RI-only model underestimates uncertainty by ignoring individual variability in slopes. Consequently, the t-value drops from a statistically suggestive level (–2.84) to a more cautious level (–1.63), demonstrating the importance of correctly specifying random effects.\nThe average effect remains the same, but the uncertainty increases, because the model properly attributes some of the variation to individual differences.\nThis is exactly what Gelman & Hill (2007) and Barr et al. (2013) argue: failing to include justified random slopes can inflate confidence in fixed effects — potentially leading to spurious conclusions.\nKey points:\nBy modeling random slopes: - We acknowledge that people learn differently over time, - We prevent overconfident or misleading group-level inferences, - And we gain access to rich insights about how variability in individual responses relates to baseline ability.\nThis modeling approach helps move us from averages to individuals, which is crucial in many health, cognitive, and behavioral domains."
  },
  {
    "objectID": "blog/2025-08-22-effect-of-RIonly_RIRS.html#take-away-message",
    "href": "blog/2025-08-22-effect-of-RIonly_RIRS.html#take-away-message",
    "title": "Why random slopes matters…",
    "section": "",
    "text": "So, what did we learn from all this?\nWe explored a progression of modeling approaches:\n\nFrom ANOVA-style pooling,\nto mass regression (no pooling),\nthen to partial pooling with a random intercept,\nand finally to a model with both random intercepts and slopes, including their correlation.\n\nEach step improved the model fit. But here’s the key insight: Only the model with random slopes gave us a trustworthy picture of individual differences.\nThe random intercept-only model, while tempting in its simplicity, produced misleading results. Specifically, it underestimated standard errors, leading to inflated Type I error rates. That’s not just a technical detail — it can fundamentally distort scientific conclusions, especially in clinical or psychological studies where understanding who improves and who doesn’t is crucial.\nSo, if you’re working with repeated measures or hierarchical data and you care about individual variability, responder/non-responder patterns, or generalization, don’t stop at random intercepts.\nModeling the variance-covariance structure of the specific data set (i.e. random terms of the mixed-effect model) aren’t just a technical flourish — they’re essential for honest modeling.\n\nWhile random intercept models are within the class of linear mixed-effect models, they are often difficult to find use cases for within cognitive neuroscience experiments.\nWhile accounting for individual starting differences is a step up from the ANOVA approach, it still assumes no variability between participants.\nFitting mass linear models (no pooling) overcomes this problem. However, researchers often aim to generalize and make population-based conclusions. It is therefore not optimal for this use case.\nLinear mixed-effect models that are modeled appropriately to a specific data set is a great approach (partial-pooling). However, they quickly become tricky to set up when there are multiple predictors, groups, covariates, and especially interactions."
  },
  {
    "objectID": "blog/2025-07-22-my-first-post.html",
    "href": "blog/2025-07-22-my-first-post.html",
    "title": "Welcome to My Blog",
    "section": "",
    "text": "Hello, world! This is my first post.\nI’ll now try to write more stuff. This should help me identify if it works…\n\nplot(1:10)"
  }
]